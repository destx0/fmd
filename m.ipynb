{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pyfhel\n",
    "!pip install pynacl\n",
    "!pip install cryptography\n",
    "!pip install tqdm\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_clients = 3\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 05:36:37.704364: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-19 05:36:37.706791: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-19 05:36:37.737976: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 05:36:38.278776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "No GPU available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 05:36:38.716569: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 05:36:38.717104: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Check if GPU is available\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "\tprint(\"GPUs available:\", len(gpus))\n",
    "\tfor gpu in gpus:\n",
    "\t\tprint(gpu)\n",
    "else:\n",
    "\tprint(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from cryptography.hazmat.primitives import hashes, serialization\n",
    "from cryptography.hazmat.primitives.asymmetric import dh\n",
    "from cryptography.hazmat.primitives.kdf.hkdf import HKDF\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from Pyfhel import Pyfhel\n",
    "import nacl.utils\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "from cryptography.hazmat.primitives import padding\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import nacl.utils\n",
    "from nacl.public import PrivateKey, SealedBox\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "\n",
    "# from src.models.FMLEE import FMLEE\n",
    "# from src.data.load_data import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save_dir = \"dataset/mnist_data/\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MAML(tf.keras.Model):\n",
    "\tdef __init__(self, model):\n",
    "\t\tsuper(MAML, self).__init__()\n",
    "\t\tself.model = model\n",
    "\n",
    "\tdef call(self, inputs):\n",
    "\t\tx = tf.reshape(inputs, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "\t\treturn self.model(x)\n",
    "\n",
    "\tdef get_config(self):\n",
    "\t\treturn {\"model\": self.model.get_config()}\n",
    "\n",
    "\t@classmethod\n",
    "\tdef from_config(cls, config):\n",
    "\t\tmodel = tf.keras.models.Model.from_config(config[\"model\"])\n",
    "\t\treturn cls(model)\n",
    "\n",
    "\tdef train_step(self, data):\n",
    "\t\tx, y = data\n",
    "\t\tx = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "\t\ty = tf.reshape(y, (-1,))  # Reshape the target labels\n",
    "\t\twith tf.GradientTape() as tape:\n",
    "\t\t\ty_pred = self.model(x)\n",
    "\t\t\tloss = self.compiled_loss(y, y_pred)\n",
    "\t\tgradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "\t\tself.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\t\tself.compiled_metrics.update_state(y, y_pred)\n",
    "\t\treturn {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\tdef test_step(self, data):\n",
    "\t\tx, y = data\n",
    "\t\tx = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "\t\ty = tf.reshape(y, (-1,))  # Reshape the target labels\n",
    "\t\ty_pred = self.model(x)\n",
    "\t\tself.compiled_loss(y, y_pred)\n",
    "\t\tself.compiled_metrics.update_state(y, y_pred)\n",
    "\t\treturn {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "num_meta_updates = 10\n",
    "num_inner_updates = 5\n",
    "meta_batch_size = 32\n",
    "inner_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMLEE:\n",
    "\tdef __init__(self, no_clients, epochs):\n",
    "\t\tself.no_clients = no_clients\n",
    "\t\tself.epochs = epochs\n",
    "\t\tprint(\"Initializing CKKS scheme...\")\n",
    "\t\tself.HE = self.CKKS()\n",
    "\t\tself.clients = []\n",
    "\t\tprint(\"Initializing clients...\")\n",
    "\t\tself.init_clients()\n",
    "\t\tprint(\"Generating asymmetric keys...\")\n",
    "\t\tself.pvt_key, self.pub_key = self.asym_keygen()\n",
    "\t\tprint(\"Initialization complete.\")\n",
    "\n",
    "\tdef model_spec(self):\n",
    "\t\tmodel = tf.keras.models.Sequential(\n",
    "\t\t\t[\n",
    "\t\t\t\ttf.keras.layers.Conv2D(\n",
    "\t\t\t\t\t32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)\n",
    "\t\t\t\t),\n",
    "\t\t\t\ttf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\t\t\t\ttf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "\t\t\t\ttf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\t\t\t\ttf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "\t\t\t\ttf.keras.layers.Flatten(),\n",
    "\t\t\t\ttf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "\t\t\t\ttf.keras.layers.Dense(10),\n",
    "\t\t\t]\n",
    "\t\t)\n",
    "\t\treturn model\n",
    "\n",
    "\tdef init_model(self):\n",
    "\t\tmodel = MAML(self.model_spec())\n",
    "\t\tmodel.compile(\n",
    "\t\t\toptimizer=tf.keras.optimizers.Adam(),\n",
    "\t\t\tloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "\t\t\tmetrics=[\"accuracy\"],\n",
    "\t\t)\n",
    "\t\treturn model\n",
    "\n",
    "\tdef CKKS(self):\n",
    "\t\tHE = Pyfhel()\n",
    "\t\tckks_params = {\n",
    "\t\t\t\"scheme\": \"CKKS\",\n",
    "\t\t\t\"n\": 2**14,  # Polynomial modulus degree. For CKKS, n/2 values can be\n",
    "\t\t\t\"scale\": 2**30,  # All the encodings will use it for float->fixed point\n",
    "\t\t\t\"qi_sizes\": [\n",
    "\t\t\t\t60,\n",
    "\t\t\t\t30,\n",
    "\t\t\t\t30,\n",
    "\t\t\t\t30,\n",
    "\t\t\t\t60,\n",
    "\t\t\t],\n",
    "\t\t}\n",
    "\t\tprint(\"Generating context for CKKS scheme...\")\n",
    "\t\tHE.contextGen(**ckks_params)  # Generate context for ckks scheme\n",
    "\t\tprint(\"Generating keys for CKKS scheme...\")\n",
    "\t\tHE.keyGen()  # Key Generation: generates a pair of public/secret keys\n",
    "\t\tHE.rotateKeyGen()\n",
    "\t\tHE.relinKeyGen()\n",
    "\t\tprint(\"CKKS scheme initialized.\")\n",
    "\t\treturn HE\n",
    "\n",
    "\tdef asym_keygen(self):\n",
    "\t\tprint(\"Generating private key...\")\n",
    "\t\tpvt_key = PrivateKey.generate()\n",
    "\t\tprint(\"Private key generated.\")\n",
    "\t\tpub_key = pvt_key.public_key\n",
    "\t\tprint(\"Public key generated.\")\n",
    "\t\treturn pvt_key, pub_key\n",
    "\n",
    "\tdef init_clients(self):\n",
    "\t\tfor i in range(self.no_clients):\n",
    "\t\t\tprint(f\"Initializing model for client {i}...\")\n",
    "\t\t\tself.clients.append(self.init_model())\n",
    "\t\t\tprint(f\"Client {i} initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_save_mnist(save_dir):\n",
    "\t(x_train_all, y_train_all), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\t# Save training data with progress bar\n",
    "\tfor array, name in zip(\n",
    "\t\t[x_train_all, y_train_all, x_test, y_test],\n",
    "\t\t[\"x_train.npy\", \"y_train.npy\", \"x_test.npy\", \"y_test.npy\"],\n",
    "\t):\n",
    "\t\twith tqdm(total=len(array), desc=f\"Saving {name}\") as pbar:\n",
    "\t\t\tnp.save(os.path.join(save_dir, name), array)\n",
    "\t\t\tpbar.update(len(array))\n",
    "\n",
    "\tprint(f\"Dataset downloaded and saved locally at {save_dir}\")\n",
    "\n",
    "\n",
    "def load_mnist_from_local(save_dir):\n",
    "\tx_train_all = np.load(os.path.join(save_dir, \"x_train.npy\"))\n",
    "\ty_train_all = np.load(os.path.join(save_dir, \"y_train.npy\"))\n",
    "\tx_test = np.load(os.path.join(save_dir, \"x_test.npy\"))\n",
    "\ty_test = np.load(os.path.join(save_dir, \"y_test.npy\"))\n",
    "\tprint(f\"Dataset loaded from local files at {save_dir}\")\n",
    "\tx_train_all = x_train_all.astype(np.float32) / 255\n",
    "\tx_test = x_test.astype(np.float32) / 255\n",
    "\n",
    "\treturn (x_train_all, y_train_all), (x_test, y_test)\n",
    "\n",
    "\n",
    "def load_mnist():\n",
    "\tif not os.path.exists(os.path.join(save_dir, \"x_train.npy\")):\n",
    "\t\tdownload_and_save_mnist(save_dir)\n",
    "\treturn load_mnist_from_local(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from local files at dataset/mnist_data/\n"
     ]
    }
   ],
   "source": [
    "(x_train_all, y_train_all), (x_test, y_test)  = load_mnist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training and test sets...\n",
      "Data split complete.\n",
      "Training set size: 48000, Temp set size: 12000, Test set size: 10000\n",
      "Splitting temp set into validation and testing sets...\n",
      "Validation and test set split complete.\n",
      "Validation set size: 10200, Test set size: 1800\n",
      "Splitting training data into 3 parts...\n",
      "Part 1 created: 16000 samples.\n",
      "Part 2 created: 16000 samples.\n",
      "Part 3 created: 16000 samples.\n",
      "Data splitting into parts complete.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load MNIST data\n",
    "(x_train_all, y_train_all), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape data\n",
    "x_train_all = x_train_all.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "# Splitting data into training and test sets\n",
    "print(\"Splitting data into training and test sets...\")\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "\tx_train_all, y_train_all, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Data split complete.\")\n",
    "print(\n",
    "\tf\"Training set size: {len(X_train)}, Temp set size: {len(X_temp)}, Test set size: {len(x_test)}\"\n",
    ")\n",
    "\n",
    "# Further split the temporary set into validation and testing sets\n",
    "print(\"Splitting temp set into validation and testing sets...\")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "\tX_temp, y_temp, test_size=0.15, random_state=42\n",
    ")\n",
    "print(\"Validation and test set split complete.\")\n",
    "print(f\"Validation set size: {len(X_val)}, Test set size: {len(X_test)}\")\n",
    "\n",
    "# Split training data into n parts\n",
    "n_parts = no_clients\n",
    "part_size = len(X_train) // n_parts\n",
    "dataset_parts = []\n",
    "\n",
    "print(f\"Splitting training data into {n_parts} parts...\")\n",
    "for i in range(n_parts):\n",
    "\tstart = i * part_size\n",
    "\tend = (i + 1) * part_size if i != n_parts - 1 else len(X_train)\n",
    "\tX_part = X_train[start:end]\n",
    "\ty_part = y_train[start:end]\n",
    "\tdataset_parts.append((X_part, y_part))\n",
    "\tprint(f\"Part {i + 1} created: {len(X_part)} samples.\")\n",
    "\n",
    "print(\"Data splitting into parts complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CKKS scheme...\n",
      "Generating context for CKKS scheme...\n",
      "Generating keys for CKKS scheme...\n",
      "CKKS scheme initialized.\n",
      "Initializing clients...\n",
      "Initializing model for client 0...\n",
      "Client 0 initialized.\n",
      "Initializing model for client 1...\n",
      "Client 1 initialized.\n",
      "Initializing model for client 2...\n",
      "Client 2 initialized.\n",
      "Generating asymmetric keys...\n",
      "Generating private key...\n",
      "Private key generated.\n",
      "Public key generated.\n",
      "Initialization complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "fml = FMLEE(no_clients, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<MAML name=maml, built=False>,\n",
       " <MAML name=maml_1, built=False>,\n",
       " <MAML name=maml_2, built=False>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fml.clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ckks Pyfhel obj at 0x75d0b013d300, [pk:Y, sk:Y, rtk:Y, rlk:Y, contx(n=16384, t=0, sec=128, qi=[60, 30, 30, 30, 60], scale=1073741824.0, )]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fml.HE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fml.clients[0].fit(x_train_all, y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Pyfhel for CKKS scheme...\n",
      "Generating context for CKKS scheme...\n",
      "Context generation complete.\n",
      "Generating public and secret keys...\n",
      "Public and secret key generation complete.\n",
      "Generating rotation keys...\n",
      "Rotation keys generation complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Pyfhel import Pyfhel\n",
    "\n",
    "\n",
    "def CKKS_keygen():\n",
    "\tprint(\"Initializing Pyfhel for CKKS scheme...\")\n",
    "\tHE = Pyfhel()\n",
    "\n",
    "\tckks_params = {\n",
    "\t\t\"scheme\": \"CKKS\",\n",
    "\t\t\"n\": 2**14,  # Polynomial modulus degree. For CKKS, n/2 values can be\n",
    "\t\t\"scale\": 2**30,  # All the encodings will use it for float->fixed point\n",
    "\t\t\"qi_sizes\": [60, 30, 30, 30, 60],  # Number of bits of each prime in the chain.\n",
    "\t}\n",
    "\n",
    "\tprint(\"Generating context for CKKS scheme...\")\n",
    "\tHE.contextGen(**ckks_params)  # Generate context for ckks scheme\n",
    "\tprint(\"Context generation complete.\")\n",
    "\n",
    "\tprint(\"Generating public and secret keys...\")\n",
    "\tHE.keyGen()  # Key Generation: generates a pair of public/secret keys\n",
    "\tprint(\"Public and secret key generation complete.\")\n",
    "\n",
    "\tprint(\"Generating rotation keys...\")\n",
    "\tHE.rotateKeyGen()\n",
    "\tprint(\"Rotation keys generation complete.\")\n",
    "\n",
    "\treturn HE\n",
    "\n",
    "\n",
    "HE = CKKS_keygen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asym_keygen():\n",
    "\tpvt_key = PrivateKey.generate()\n",
    "\tpub_key = pvt_key.public_key\n",
    "\treturn pvt_key, pub_key\n",
    "\n",
    "\n",
    "agg_pvt_key, agg_pub_key = asym_keygen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nacl_session_keygen():\n",
    "\treturn nacl.utils.random(32)\n",
    "\n",
    "def encrypt_symmetric_key(pub_key, symmetric_key):\n",
    "\tsealed_box = SealedBox(pub_key)\n",
    "\treturn sealed_box.encrypt(symmetric_key)\n",
    "\n",
    "def decrypt_symmetric_key(pvt_key, encrypted_key):\n",
    "\tsealed_box = SealedBox(pvt_key)\n",
    "\treturn sealed_box.decrypt(encrypted_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def maml_train_step(model, x_train, y_train, inner_lr, num_inner_updates):\n",
    "\n",
    "\tmodel.fit(\n",
    "\t\t\tx_train,\n",
    "\t\t\ty_train,\n",
    "\t\t\tepochs=1,\n",
    "\t\t\tbatch_size=64,\n",
    "\t\t\tverbose=1,\n",
    "\t\t\tvalidation_data=(x_train, y_train),\n",
    "\t\t)\n",
    "\twith tf.GradientTape() as outer_tape:\n",
    "\t\tfor i in range(num_inner_updates):\n",
    "\t\t\twith tf.GradientTape() as inner_tape:\n",
    "\t\t\t\tpredictions = model(x_train, training=True)\n",
    "\t\t\t\tloss = tf.reduce_mean(\n",
    "\t\t\t\t\ttf.keras.losses.sparse_categorical_crossentropy(\n",
    "\t\t\t\t\t\ty_train, predictions\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t)\n",
    "\t\t\tgrads = inner_tape.gradient(loss, model.trainable_variables)\n",
    "\t\t\tfor var, grad in zip(model.trainable_variables, grads):\n",
    "\t\t\t\tif grad is not None:\n",
    "\t\t\t\t\tvar.assign_sub(inner_lr * grad)\n",
    "\n",
    "\t\tpredictions = model(x_train, training=True)\n",
    "\t\touter_loss = tf.reduce_mean(\n",
    "\t\t\ttf.keras.losses.sparse_categorical_crossentropy(y_train, predictions)\n",
    "\t\t)\n",
    "\n",
    "\touter_grads = outer_tape.gradient(outer_loss, model.trainable_variables)\n",
    "\treturn outer_loss, outer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HE_encrypt(wtarray):\n",
    "\tcwt = []\n",
    "\tfor layer in wtarray:\n",
    "\t\tflat_array = layer.astype(np.float64).flatten()\n",
    "\n",
    "\t\tchunks = np.array_split(flat_array, (len(flat_array) + 2**10 - 1) // 2**10)\n",
    "\t\tclayer = []\n",
    "\t\t\n",
    "\t\tfor chunk in chunks:\n",
    "\t\t\tptxt = HE.encodeFrac(chunk)\n",
    "\t\t\tctxt = HE.encryptPtxt(ptxt)\n",
    "\t\t\tclayer.append(ctxt)\n",
    "\t\tcwt.append(clayer.copy())\n",
    "\t\t\n",
    "\treturn cwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_message_sym_AES(key, message):\n",
    "\tserialized_obj = pickle.dumps(message)\n",
    "\n",
    "\tiv = nacl.utils.random(16)\n",
    "\n",
    "\tpadder = padding.PKCS7(algorithms.AES.block_size).padder()\n",
    "\tpadded_obj = padder.update(serialized_obj) + padder.finalize()\n",
    "\n",
    "\tcipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n",
    "\tencryptor = cipher.encryptor()\n",
    "\tciphertext = encryptor.update(padded_obj) + encryptor.finalize()\n",
    "\n",
    "\treturn iv + ciphertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_message_sym_AES(key , ciphertext):\n",
    "\tiv = ciphertext[:16]\n",
    "\tciphertext = ciphertext[16:]\n",
    "\n",
    "\tcipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n",
    "\tdecryptor = cipher.decryptor()\n",
    "\tpadded_obj = decryptor.update(ciphertext) + decryptor.finalize()\n",
    "\n",
    "\tunpadder = padding.PKCS7(algorithms.AES.block_size).unpadder()\n",
    "\tunpadded_obj = unpadder.update(padded_obj) + unpadder.finalize()\n",
    "\n",
    "\treturn pickle.loads(unpadded_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wts_ckks(encrypted_wts):\n",
    "\tres_wts = []\n",
    "\tprint(len(encrypted_wts) )\n",
    "\tprint(len(encrypted_wts[0]) )\n",
    "\tfor j in range(len(encrypted_wts[0])):\n",
    "\t\tlayer = []\n",
    "\t\tfor k in range(len(encrypted_wts[0][j])):\n",
    "\t\t\ttmp = encrypted_wts[0][j][k].copy()\n",
    "\n",
    "\t\t\tfor i in range(1, len(encrypted_wts)):\n",
    "\t\t\t\ttmp = tmp + encrypted_wts[i][j][k]\n",
    "\n",
    "\t\t\ttmp = tmp / len(encrypted_wts)\n",
    "\t\t\tlayer.append(tmp)\n",
    "\n",
    "\t\tres_wts.append(layer.copy())\n",
    "\t\t\n",
    "\treturn res_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_wts_ckks(encrypted_wts):\n",
    "\tdecrypted_wts = []\n",
    "\twtarray = dummy_model.get_weights()\n",
    "\n",
    "\tfor layer_wts , layer in zip(encrypted_wts , wtarray):\n",
    "\t\tdecrypted_layer = []\n",
    "\t\tflat_array = layer.astype(np.float64).flatten()\n",
    "\t\tchunks = np.array_split(flat_array, (len(flat_array) + 2**13 - 1) // 2**13)\n",
    "\n",
    "\t\tfor chunk , cchunk in zip(chunks , layer_wts):\n",
    "\t\t\tdecrypted_chunk = HE.decryptFrac(cchunk)\n",
    "\t\t\toriginal_chunk_size  = len(chunk)\n",
    "\t\t\tdecrypted_chunk = decrypted_chunk[:original_chunk_size]\n",
    "\t\t\t\n",
    "\t\t\tdecrypted_layer.append(decrypted_chunk)\n",
    "\t\t\t\n",
    "\t\tdecrypted_layer = np.concatenate(decrypted_layer)\n",
    "\t\tdecrypted_layer = decrypted_layer.reshape(layer.shape)\n",
    "\t\tdecrypted_wts.append(decrypted_layer)\n",
    "\t\t\n",
    "\treturn decrypted_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wts(wts):\n",
    "\tpeeled_wts = []\n",
    "\tfor client_id in range(no_clients):\n",
    "\t\tsesion_key = decrypt_symmetric_key(agg_pvt_key , agg_sesion_keys[client_id])\n",
    "\t\tpeeled_wt = decrypt_message_sym_AES(sesion_key, wts[client_id])\n",
    "\t\tpeeled_wts.append(peeled_wt)\n",
    "\tres_wts = aggregate_wts_ckks(peeled_wts)\n",
    "\treturn res_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = fml.clients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_sesion_keys = [0 for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_lr = 0.001\n",
    "num_inner_updates = 1\n",
    "outer_lr = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [[] for i in range(no_clients)]\n",
    "losses = [[] for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_session_keys = [0 for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_wts = [0 for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:603: UserWarning: `model.compiled_loss()` is deprecated. Instead, use `model.compute_loss(x, y, y_pred, sample_weight)`.\n",
      "  warnings.warn(\n",
      "/home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:578: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n",
      "```\n",
      "for metric in self.metrics:\n",
      "    metric.update_state(y, y_pred)\n",
      "```\n",
      "\n",
      "  return self._compiled_metrics_update_state(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.7431 - loss: 0.3630 - val_accuracy: 0.9572 - val_loss: 0.2976\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.7296 - loss: 0.1799 - val_accuracy: 0.9650 - val_loss: 0.1372\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.7251 - loss: -0.1036 - val_accuracy: 0.9626 - val_loss: -0.4770\n",
      "101725776\n",
      "3\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:49<07:24, 49.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6302 - loss: -0.2458 - val_accuracy: 0.9246 - val_loss: -0.3440\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.5838 - loss: -0.2307 - val_accuracy: 0.9374 - val_loss: -0.4301\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.6112 - loss: -0.2879 - val_accuracy: 0.9394 - val_loss: -0.8031\n",
      "101725776\n",
      "3\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:35<06:18, 47.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6028 - loss: -0.3868 - val_accuracy: 0.9396 - val_loss: -0.8058\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.4974 - loss: -0.6629 - val_accuracy: 0.9137 - val_loss: -1.6404\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6063 - loss: -0.1142 - val_accuracy: 0.9310 - val_loss: -0.6159\n",
      "101725776\n",
      "3\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [02:23<05:33, 47.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.5853 - loss: -0.6491 - val_accuracy: 0.9337 - val_loss: -2.0254\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.5627 - loss: -0.6944 - val_accuracy: 0.9398 - val_loss: -1.6270\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5753 - loss: -0.5175 - val_accuracy: 0.9291 - val_loss: -0.9606\n",
      "101725776\n",
      "3\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [03:10<04:44, 47.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.6052 - loss: -0.7328 - val_accuracy: 0.9411 - val_loss: -1.3578\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6091 - loss: -0.9618 - val_accuracy: 0.9523 - val_loss: -1.7605\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.6050 - loss: -1.4147 - val_accuracy: 0.9512 - val_loss: -2.6893\n",
      "101725776\n",
      "3\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [03:56<03:54, 46.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6222 - loss: -1.3797 - val_accuracy: 0.9474 - val_loss: -2.2614\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.6010 - loss: -1.3084 - val_accuracy: 0.9421 - val_loss: -2.3625\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.6235 - loss: -1.5969 - val_accuracy: 0.9409 - val_loss: -3.3987\n",
      "101725776\n",
      "3\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [04:43<03:07, 46.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6065 - loss: -1.3906 - val_accuracy: 0.9524 - val_loss: -2.6075\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6308 - loss: -1.4935 - val_accuracy: 0.9490 - val_loss: -2.5858\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6195 - loss: -1.2065 - val_accuracy: 0.9466 - val_loss: -1.8785\n",
      "101725776\n",
      "3\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [05:30<02:20, 46.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6341 - loss: -1.5272 - val_accuracy: 0.9536 - val_loss: -2.5736\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6177 - loss: -1.5413 - val_accuracy: 0.9553 - val_loss: -2.7113\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6417 - loss: -1.8229 - val_accuracy: 0.9574 - val_loss: -3.6103\n",
      "101725776\n",
      "3\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [06:16<01:33, 46.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6511 - loss: -1.8243 - val_accuracy: 0.9538 - val_loss: -3.2177\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6369 - loss: -1.7614 - val_accuracy: 0.9494 - val_loss: -3.0387\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6410 - loss: -1.6765 - val_accuracy: 0.9474 - val_loss: -3.0039\n",
      "101725776\n",
      "3\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [07:03<00:46, 46.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6742 - loss: -2.0014 - val_accuracy: 0.9539 - val_loss: -3.3521\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6843 - loss: -2.0727 - val_accuracy: 0.9564 - val_loss: -3.1963\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6742 - loss: -2.1214 - val_accuracy: 0.9457 - val_loss: -3.3961\n",
      "101725776\n",
      "3\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [07:49<00:00, 46.95s/it]\n"
     ]
    }
   ],
   "source": [
    "for r in tqdm(range(epochs)):\n",
    "\tfor client_id , (client , client_dataset) in enumerate(zip(fml.clients , dataset_parts)):\n",
    "\t\tmodel = client\n",
    "\t\tx_train, y_train = client_dataset\n",
    "\n",
    "\t\touter_loss, outer_grads = maml_train_step(\n",
    "\t\t\t\tmodel, x_train, y_train, inner_lr, num_inner_updates\n",
    "\t\t\t)\n",
    "\t\toptimizer = tf.keras.optimizers.Adam(learning_rate=outer_lr)\n",
    "\t\toptimizer.apply_gradients(zip(outer_grads, model.trainable_variables))\n",
    "\t\thistory = model.evaluate(\n",
    "\t\t\tx_train,\n",
    "\t\t\ty_train,\n",
    "\t\t\tbatch_size=64,\n",
    "\t\t\tverbose=0,\n",
    "\t\t)\n",
    "\t\taccuracies[client_id].append(history[1])\n",
    "\t\tlosses[client_id].append(history[0])\n",
    "\t\ttrained_weights = model.get_weights()\n",
    "\n",
    "\t\tsession_key = nacl_session_keygen()\n",
    "\t\tclient_session_keys[client_id] = session_key\n",
    "\t\tenc_session_key = encrypt_symmetric_key(agg_pub_key , session_key)\n",
    "\t\tagg_sesion_keys[client_id] = enc_session_key\n",
    "\n",
    "\t\tHe_ciphertext = HE_encrypt(trained_weights)\n",
    "\t\tsym_ctxt = encrypt_message_sym_AES(session_key , He_ciphertext)\n",
    "\t\tenc_wts[client_id] = sym_ctxt\n",
    "\tprint(len(enc_wts[0]))\n",
    "\tagg_wts = aggregate_wts(enc_wts)\n",
    "\n",
    "\tfor client_id , client in enumerate(fml.clients):\n",
    "\t\tnew_wts = decrypt_wts_ckks(agg_wts)        \n",
    "\n",
    "\t\tclient.set_weights(new_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3.8595826625823975, 0.9488750100135803]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_parts[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.9350000023841858,\n",
       "  0.9244999885559082,\n",
       "  0.9206249713897705,\n",
       "  0.9281874895095825,\n",
       "  0.8998125195503235,\n",
       "  0.9438124895095825,\n",
       "  0.9478750228881836,\n",
       "  0.9398750066757202,\n",
       "  0.9466249942779541,\n",
       "  0.9545624852180481],\n",
       " [0.9488124847412109,\n",
       "  0.9305624961853027,\n",
       "  0.8976250290870667,\n",
       "  0.902999997138977,\n",
       "  0.9259999990463257,\n",
       "  0.9404374957084656,\n",
       "  0.9474375247955322,\n",
       "  0.9514999985694885,\n",
       "  0.9475625157356262,\n",
       "  0.9520624876022339],\n",
       " [0.9415000081062317,\n",
       "  0.9151874780654907,\n",
       "  0.9090625047683716,\n",
       "  0.9029374718666077,\n",
       "  0.9317499995231628,\n",
       "  0.9362499713897705,\n",
       "  0.9316250085830688,\n",
       "  0.953499972820282,\n",
       "  0.9503124952316284,\n",
       "  0.9488750100135803]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "he39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
