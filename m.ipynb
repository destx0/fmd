{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pyfhel in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.21 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from Pyfhel) (1.26.4)\n",
      "Requirement already satisfied: pynacl in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (1.5.0)\n",
      "Requirement already satisfied: cffi>=1.4.1 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from pynacl) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from cffi>=1.4.1->pynacl) (2.22)\n",
      "Requirement already satisfied: cryptography in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (42.0.8)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from cryptography) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from cffi>=1.12->cryptography) (2.22)\n",
      "Requirement already satisfied: tqdm in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (4.66.4)\n",
      "Requirement already satisfied: scikit-learn in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Pyfhel\n",
    "!pip install pynacl\n",
    "!pip install cryptography\n",
    "!pip install tqdm\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_clients = 3\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Check if GPU is available\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "\tprint(\"GPUs available:\", len(gpus))\n",
    "\tfor gpu in gpus:\n",
    "\t\tprint(gpu)\n",
    "else:\n",
    "\tprint(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from cryptography.hazmat.primitives import hashes, serialization\n",
    "from cryptography.hazmat.primitives.asymmetric import dh\n",
    "from cryptography.hazmat.primitives.kdf.hkdf import HKDF\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from Pyfhel import Pyfhel\n",
    "import nacl.utils\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "from cryptography.hazmat.primitives import padding\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import nacl.utils\n",
    "from nacl.public import PrivateKey, SealedBox\n",
    "# from src.models.FMLEE import FMLEE\n",
    "# from src.data.load_data import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save_dir = \"dataset/mnist_data/\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MAML(tf.keras.Model):\n",
    "\tdef __init__(self, model):\n",
    "\t\tsuper(MAML, self).__init__()\n",
    "\t\tself.model = model\n",
    "\n",
    "\tdef call(self, inputs):\n",
    "\t\tx = tf.reshape(inputs, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "\t\treturn self.model(x)\n",
    "\n",
    "\tdef get_config(self):\n",
    "\t\treturn {\"model\": self.model.get_config()}\n",
    "\n",
    "\t@classmethod\n",
    "\tdef from_config(cls, config):\n",
    "\t\tmodel = tf.keras.models.Model.from_config(config[\"model\"])\n",
    "\t\treturn cls(model)\n",
    "\n",
    "\tdef train_step(self, data):\n",
    "\t\tx, y = data\n",
    "\t\tx = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "\t\ty = tf.reshape(y, (-1,))  # Reshape the target labels\n",
    "\t\twith tf.GradientTape() as tape:\n",
    "\t\t\ty_pred = self.model(x)\n",
    "\t\t\tloss = self.compiled_loss(y, y_pred)\n",
    "\t\tgradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "\t\tself.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\t\tself.compiled_metrics.update_state(y, y_pred)\n",
    "\t\treturn {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\tdef test_step(self, data):\n",
    "\t\tx, y = data\n",
    "\t\tx = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "\t\ty = tf.reshape(y, (-1,))  # Reshape the target labels\n",
    "\t\ty_pred = self.model(x)\n",
    "\t\tself.compiled_loss(y, y_pred)\n",
    "\t\tself.compiled_metrics.update_state(y, y_pred)\n",
    "\t\treturn {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "num_meta_updates = 10\n",
    "num_inner_updates = 5\n",
    "meta_batch_size = 32\n",
    "inner_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMLEE:\n",
    "\tdef __init__(self, no_clients, epochs):\n",
    "\t\tself.no_clients = no_clients\n",
    "\t\tself.epochs = epochs\n",
    "\t\tprint(\"Initializing CKKS scheme...\")\n",
    "\t\tself.HE = self.CKKS()\n",
    "\t\tself.clients = []\n",
    "\t\tprint(\"Initializing clients...\")\n",
    "\t\tself.init_clients()\n",
    "\t\tprint(\"Generating asymmetric keys...\")\n",
    "\t\tself.pvt_key, self.pub_key = self.asym_keygen()\n",
    "\t\tprint(\"Initialization complete.\")\n",
    "\n",
    "\tdef model_spec(self):\n",
    "\t\tmodel = tf.keras.models.Sequential(\n",
    "\t\t\t[\n",
    "\t\t\t\ttf.keras.layers.Conv2D(\n",
    "\t\t\t\t\t32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)\n",
    "\t\t\t\t),\n",
    "\t\t\t\ttf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\t\t\t\ttf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "\t\t\t\ttf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\t\t\t\ttf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "\t\t\t\ttf.keras.layers.Flatten(),\n",
    "\t\t\t\ttf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "\t\t\t\ttf.keras.layers.Dense(10),\n",
    "\t\t\t]\n",
    "\t\t)\n",
    "\t\treturn model\n",
    "\n",
    "\tdef init_model(self):\n",
    "\t\tmodel = MAML(self.model_spec())\n",
    "\t\tmodel.compile(\n",
    "\t\t\toptimizer=tf.keras.optimizers.Adam(),\n",
    "\t\t\tloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "\t\t\tmetrics=[\"accuracy\"],\n",
    "\t\t)\n",
    "\t\treturn model\n",
    "\n",
    "\tdef CKKS(self):\n",
    "\t\tHE = Pyfhel()\n",
    "\t\tckks_params = {\n",
    "\t\t\t\"scheme\": \"CKKS\",\n",
    "\t\t\t\"n\": 2**14,  # Polynomial modulus degree. For CKKS, n/2 values can be\n",
    "\t\t\t\"scale\": 2**30,  # All the encodings will use it for float->fixed point\n",
    "\t\t\t\"qi_sizes\": [\n",
    "\t\t\t\t60,\n",
    "\t\t\t\t30,\n",
    "\t\t\t\t30,\n",
    "\t\t\t\t30,\n",
    "\t\t\t\t60,\n",
    "\t\t\t],\n",
    "\t\t}\n",
    "\t\tprint(\"Generating context for CKKS scheme...\")\n",
    "\t\tHE.contextGen(**ckks_params)  # Generate context for ckks scheme\n",
    "\t\tprint(\"Generating keys for CKKS scheme...\")\n",
    "\t\tHE.keyGen()  # Key Generation: generates a pair of public/secret keys\n",
    "\t\tHE.rotateKeyGen()\n",
    "\t\tHE.relinKeyGen()\n",
    "\t\tprint(\"CKKS scheme initialized.\")\n",
    "\t\treturn HE\n",
    "\n",
    "\tdef asym_keygen(self):\n",
    "\t\tprint(\"Generating private key...\")\n",
    "\t\tpvt_key = PrivateKey.generate()\n",
    "\t\tprint(\"Private key generated.\")\n",
    "\t\tpub_key = pvt_key.public_key\n",
    "\t\tprint(\"Public key generated.\")\n",
    "\t\treturn pvt_key, pub_key\n",
    "\n",
    "\tdef init_clients(self):\n",
    "\t\tfor i in range(self.no_clients):\n",
    "\t\t\tprint(f\"Initializing model for client {i}...\")\n",
    "\t\t\tself.clients.append(self.init_model())\n",
    "\t\t\tprint(f\"Client {i} initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_save_mnist(save_dir):\n",
    "\t(x_train_all, y_train_all), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\t# Save training data with progress bar\n",
    "\tfor array, name in zip(\n",
    "\t\t[x_train_all, y_train_all, x_test, y_test],\n",
    "\t\t[\"x_train.npy\", \"y_train.npy\", \"x_test.npy\", \"y_test.npy\"],\n",
    "\t):\n",
    "\t\twith tqdm(total=len(array), desc=f\"Saving {name}\") as pbar:\n",
    "\t\t\tnp.save(os.path.join(save_dir, name), array)\n",
    "\t\t\tpbar.update(len(array))\n",
    "\n",
    "\tprint(f\"Dataset downloaded and saved locally at {save_dir}\")\n",
    "\n",
    "\n",
    "def load_mnist_from_local(save_dir):\n",
    "\tx_train_all = np.load(os.path.join(save_dir, \"x_train.npy\"))\n",
    "\ty_train_all = np.load(os.path.join(save_dir, \"y_train.npy\"))\n",
    "\tx_test = np.load(os.path.join(save_dir, \"x_test.npy\"))\n",
    "\ty_test = np.load(os.path.join(save_dir, \"y_test.npy\"))\n",
    "\tprint(f\"Dataset loaded from local files at {save_dir}\")\n",
    "\tx_train_all = x_train_all.astype(np.float32) / 255\n",
    "\tx_test = x_test.astype(np.float32) / 255\n",
    "\n",
    "\treturn (x_train_all, y_train_all), (x_test, y_test)\n",
    "\n",
    "\n",
    "def load_mnist():\n",
    "\tif not os.path.exists(os.path.join(save_dir, \"x_train.npy\")):\n",
    "\t\tdownload_and_save_mnist(save_dir)\n",
    "\treturn load_mnist_from_local(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from local files at dataset/mnist_data/\n"
     ]
    }
   ],
   "source": [
    "(x_train_all, y_train_all), (x_test, y_test)  = load_mnist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training and test sets...\n",
      "Data split complete.\n",
      "Training set size: 48000, Temp set size: 12000, Test set size: 10000\n",
      "Splitting temp set into validation and testing sets...\n",
      "Validation and test set split complete.\n",
      "Validation set size: 10200, Test set size: 1800\n",
      "Splitting training data into 3 parts...\n",
      "Part 1 created: 16000 samples.\n",
      "Part 2 created: 16000 samples.\n",
      "Part 3 created: 16000 samples.\n",
      "Data splitting into parts complete.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load MNIST data\n",
    "(x_train_all, y_train_all), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape data\n",
    "x_train_all = x_train_all.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "# Splitting data into training and test sets\n",
    "print(\"Splitting data into training and test sets...\")\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "\tx_train_all, y_train_all, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Data split complete.\")\n",
    "print(\n",
    "\tf\"Training set size: {len(X_train)}, Temp set size: {len(X_temp)}, Test set size: {len(x_test)}\"\n",
    ")\n",
    "\n",
    "# Further split the temporary set into validation and testing sets\n",
    "print(\"Splitting temp set into validation and testing sets...\")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "\tX_temp, y_temp, test_size=0.15, random_state=42\n",
    ")\n",
    "print(\"Validation and test set split complete.\")\n",
    "print(f\"Validation set size: {len(X_val)}, Test set size: {len(X_test)}\")\n",
    "\n",
    "# Split training data into n parts\n",
    "n_parts = no_clients\n",
    "part_size = len(X_train) // n_parts\n",
    "dataset_parts = []\n",
    "\n",
    "print(f\"Splitting training data into {n_parts} parts...\")\n",
    "for i in range(n_parts):\n",
    "\tstart = i * part_size\n",
    "\tend = (i + 1) * part_size if i != n_parts - 1 else len(X_train)\n",
    "\tX_part = X_train[start:end]\n",
    "\ty_part = y_train[start:end]\n",
    "\tdataset_parts.append((X_part, y_part))\n",
    "\tprint(f\"Part {i + 1} created: {len(X_part)} samples.\")\n",
    "\n",
    "print(\"Data splitting into parts complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CKKS scheme...\n",
      "Generating context for CKKS scheme...\n",
      "Generating keys for CKKS scheme...\n",
      "CKKS scheme initialized.\n",
      "Initializing clients...\n",
      "Initializing model for client 0...\n",
      "Client 0 initialized.\n",
      "Initializing model for client 1...\n",
      "Client 1 initialized.\n",
      "Initializing model for client 2...\n",
      "Client 2 initialized.\n",
      "Generating asymmetric keys...\n",
      "Generating private key...\n",
      "Private key generated.\n",
      "Public key generated.\n",
      "Initialization complete.\n"
     ]
    }
   ],
   "source": [
    "fml = FMLEE(no_clients, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<MAML name=maml_6, built=False>,\n",
       " <MAML name=maml_7, built=False>,\n",
       " <MAML name=maml_8, built=False>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fml.clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ckks Pyfhel obj at 0x79c32e06a760, [pk:Y, sk:Y, rtk:Y, rlk:Y, contx(n=16384, t=0, sec=128, qi=[60, 30, 30, 30, 60], scale=1073741824.0, )]>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fml.HE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fml.clients[0].fit(x_train_all, y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Pyfhel for CKKS scheme...\n",
      "Generating context for CKKS scheme...\n",
      "Context generation complete.\n",
      "Generating public and secret keys...\n",
      "Public and secret key generation complete.\n",
      "Generating rotation keys...\n",
      "Rotation keys generation complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Pyfhel import Pyfhel\n",
    "\n",
    "\n",
    "def CKKS_keygen():\n",
    "\tprint(\"Initializing Pyfhel for CKKS scheme...\")\n",
    "\tHE = Pyfhel()\n",
    "\n",
    "\tckks_params = {\n",
    "\t\t\"scheme\": \"CKKS\",\n",
    "\t\t\"n\": 2**14,  # Polynomial modulus degree. For CKKS, n/2 values can be\n",
    "\t\t\"scale\": 2**30,  # All the encodings will use it for float->fixed point\n",
    "\t\t\"qi_sizes\": [60, 30, 30, 30, 60],  # Number of bits of each prime in the chain.\n",
    "\t}\n",
    "\n",
    "\tprint(\"Generating context for CKKS scheme...\")\n",
    "\tHE.contextGen(**ckks_params)  # Generate context for ckks scheme\n",
    "\tprint(\"Context generation complete.\")\n",
    "\n",
    "\tprint(\"Generating public and secret keys...\")\n",
    "\tHE.keyGen()  # Key Generation: generates a pair of public/secret keys\n",
    "\tprint(\"Public and secret key generation complete.\")\n",
    "\n",
    "\tprint(\"Generating rotation keys...\")\n",
    "\tHE.rotateKeyGen()\n",
    "\tprint(\"Rotation keys generation complete.\")\n",
    "\n",
    "\treturn HE\n",
    "\n",
    "\n",
    "HE = CKKS_keygen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asym_keygen():\n",
    "\tpvt_key = PrivateKey.generate()\n",
    "\tpub_key = pvt_key.public_key\n",
    "\treturn pvt_key, pub_key\n",
    "\n",
    "\n",
    "agg_pvt_key, agg_pub_key = asym_keygen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nacl_session_keygen():\n",
    "\treturn nacl.utils.random(32)\n",
    "\n",
    "def encrypt_symmetric_key(pub_key, symmetric_key):\n",
    "\tsealed_box = SealedBox(pub_key)\n",
    "\treturn sealed_box.encrypt(symmetric_key)\n",
    "\n",
    "def decrypt_symmetric_key(pvt_key, encrypted_key):\n",
    "\tsealed_box = SealedBox(pvt_key)\n",
    "\treturn sealed_box.decrypt(encrypted_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def maml_train_step(model, x_train, y_train, inner_lr, num_inner_updates):\n",
    "\n",
    "\tmodel.fit(\n",
    "\t\t\tx_train,\n",
    "\t\t\ty_train,\n",
    "\t\t\tepochs=1,\n",
    "\t\t\tbatch_size=64,\n",
    "\t\t\tverbose=1,\n",
    "\t\t\tvalidation_data=(x_train, y_train),\n",
    "\t\t)\n",
    "\twith tf.GradientTape() as outer_tape:\n",
    "\t\tfor i in range(num_inner_updates):\n",
    "\t\t\twith tf.GradientTape() as inner_tape:\n",
    "\t\t\t\tpredictions = model(x_train, training=True)\n",
    "\t\t\t\tloss = tf.reduce_mean(\n",
    "\t\t\t\t\ttf.keras.losses.sparse_categorical_crossentropy(\n",
    "\t\t\t\t\t\ty_train, predictions\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t)\n",
    "\t\t\tgrads = inner_tape.gradient(loss, model.trainable_variables)\n",
    "\t\t\tfor var, grad in zip(model.trainable_variables, grads):\n",
    "\t\t\t\tif grad is not None:\n",
    "\t\t\t\t\tvar.assign_sub(inner_lr * grad)\n",
    "\n",
    "\t\tpredictions = model(x_train, training=True)\n",
    "\t\touter_loss = tf.reduce_mean(\n",
    "\t\t\ttf.keras.losses.sparse_categorical_crossentropy(y_train, predictions)\n",
    "\t\t)\n",
    "\n",
    "\touter_grads = outer_tape.gradient(outer_loss, model.trainable_variables)\n",
    "\treturn outer_loss, outer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HE_encrypt(wtarray):\n",
    "    cwt = []\n",
    "    for layer in wtarray:\n",
    "        flat_array = layer.astype(np.float64).flatten()\n",
    "\n",
    "        chunks = np.array_split(flat_array, (len(flat_array) + 2**10 - 1) // 2**10)\n",
    "        clayer = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            ptxt = HE.encodeFrac(chunk)\n",
    "            ctxt = HE.encryptPtxt(ptxt)\n",
    "            clayer.append(ctxt)\n",
    "        cwt.append(clayer.copy())\n",
    "        \n",
    "    return ciphertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_message_sym_AES(key, message):\n",
    "    serialized_obj = pickle.dumps(message)\n",
    "\n",
    "    iv = nacl.utils.random(16)\n",
    "\n",
    "    padder = padding.PKCS7(algorithms.AES.block_size).padder()\n",
    "    padded_obj = padder.update(serialized_obj) + padder.finalize()\n",
    "\n",
    "    cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n",
    "    encryptor = cipher.encryptor()\n",
    "    ciphertext = encryptor.update(padded_obj) + encryptor.finalize()\n",
    "\n",
    "    return iv + ciphertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_message_sym_AES(key , ciphertext):\n",
    "    iv = ciphertext[:16]\n",
    "    ciphertext = ciphertext[16:]\n",
    "\n",
    "    cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n",
    "    decryptor = cipher.decryptor()\n",
    "    padded_obj = decryptor.update(ciphertext) + decryptor.finalize()\n",
    "\n",
    "    unpadder = padding.PKCS7(algorithms.AES.block_size).unpadder()\n",
    "    unpadded_obj = unpadder.update(padded_obj) + unpadder.finalize()\n",
    "\n",
    "    return pickle.loads(unpadded_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wts_ckks(encrypted_wts):\n",
    "    res_wts = []\n",
    "\n",
    "    for j in range(len(encrypted_wts[0])):\n",
    "        layer = []\n",
    "        for k in range(len(encrypted_wts[0][j])):\n",
    "            tmp = encrypted_wts[0][j][k].copy()\n",
    "\n",
    "            for i in range(1, len(encrypted_wts)):\n",
    "                tmp = temp + encrypted_wts[i][j][k]\n",
    "\n",
    "            tmp = tmp / len(encrypted_wts)\n",
    "            layer.append(tmp)\n",
    "\n",
    "        res_wts.append(layer.copy())\n",
    "        \n",
    "    return res_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_wts_ckks(encrypted_wts):\n",
    "    decrypted_wts = []\n",
    "    wtarray = dummy_model.get_weights()\n",
    "\n",
    "    for layer_wts , layer in zip(encrypted_wts , wtarray):\n",
    "        decrypted_layer = []\n",
    "        flat_array = layer.astype(np.float64).flatten()\n",
    "        chunks = np.array_split(flat_array, (len(flat_array) + 2**13 - 1) // 2**13)\n",
    "\n",
    "        for chunk , cchunk in zip(chunks , layer_wts):\n",
    "            decrypted_chunk = HE.decryptFrac(cchunk)\n",
    "            original_chunk_size  = len(chunk)\n",
    "            decrypted_chunk = decrypted_chunk[:original_chunk_size]\n",
    "            \n",
    "            decrypted_layer.append(decrypted_chunk)\n",
    "            \n",
    "        decrypted_layer = np.concatenate(decrypted_layer)\n",
    "        decrypted_layer = decrypted_layer.reshape(layer.shape)\n",
    "        decrypted_wts.append(decrypted_layer)\n",
    "        \n",
    "    return decrypted_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wts(wts):\n",
    "    peeled_wts = []\n",
    "    for client_id in range(no_clients):\n",
    "        sesion_key = decrypt_symmetric_key(agg_pvt_key , agg_sesion_keys[client_id])\n",
    "        peeled_wt = decrypt_message_sym_AES(sesion_key, wts[client_id])\n",
    "    \n",
    "    res_wts = aggregate_wts_ckks(peeled_wts)\n",
    "    return res_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = fml.clients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_sesion_keys = [0 for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_lr = 0.001\n",
    "num_inner_updates = 1\n",
    "outer_lr = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [[] for i in range(no_clients)]\n",
    "losses = [[] for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_session_keys = [0 for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m client_id , (client , client_dataset) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(fml\u001b[38;5;241m.\u001b[39mclients , dataset_parts)):\n\u001b[1;32m      3\u001b[0m         model \u001b[38;5;241m=\u001b[39m client\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "for r in tqdm(range(epochs)):\n",
    "    for client_id , (client , client_dataset) in enumerate(zip(fml.clients , dataset_parts)):\n",
    "        model = client\n",
    "        x_train, y_train = client_dataset\n",
    "\n",
    "        outer_loss, outer_grads = maml_train_step(\n",
    "\t\t\t\tmodel, x_train, y_train, inner_lr, num_inner_updates\n",
    "\t\t\t)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=outer_lr)\n",
    "        optimizer.apply_gradients(zip(outer_grads, model.trainable_variables))\n",
    "        history = model.evaluate(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            batch_size=64,\n",
    "            verbose=0,\n",
    "        )\n",
    "        accuracies[client_id].append(history[1])\n",
    "        losses[client_id].append(history[0])\n",
    "        trained_weights = model.get_weights()\n",
    "\n",
    "        session_key = nacl_session_keygen()\n",
    "        client_session_keys[client_id] = session_key\n",
    "        enc_session_key = encrypt_symmetric_key(agg_pub_key , session_key)\n",
    "        agg_sesion_keys[client_id] = enc_session_key\n",
    "\n",
    "        He_ciphertext = HE_encrypt(trained_weights)\n",
    "        sym_ctxt = encrypt_message_sym_AES(session_key , He_ciphertext)\n",
    "        enc_wts[client_id] = sym_ctxt\n",
    "        \n",
    "    agg_wts = agggregate_wts()\n",
    "    \n",
    "    for client_id , client in enumerate(fml.clients):\n",
    "        new_wts = decrypt_wts_ckks(agg_wts)        \n",
    "\n",
    "        client.set_weights(new_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3.5314900875091553, 0.9926249980926514]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_parts[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.9706249833106995, 0.9886875152587891, 0.992562472820282],\n",
       " [0.9772499799728394, 0.9846875071525574, 0.9941874742507935],\n",
       " [0.9736250042915344, 0.9865000247955322, 0.9913125038146973]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "he39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
