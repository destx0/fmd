{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pyfhel in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.21 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from Pyfhel) (1.26.4)\n",
      "Requirement already satisfied: pynacl in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (1.5.0)\n",
      "Requirement already satisfied: cffi>=1.4.1 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from pynacl) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from cffi>=1.4.1->pynacl) (2.22)\n",
      "Requirement already satisfied: cryptography in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (42.0.8)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from cryptography) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from cffi>=1.12->cryptography) (2.22)\n",
      "Requirement already satisfied: tqdm in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (4.66.4)\n",
      "Requirement already satisfied: scikit-learn in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Pyfhel\n",
    "!pip install pynacl\n",
    "!pip install cryptography\n",
    "!pip install tqdm\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_clients = 3\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Check if GPU is available\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "\tprint(\"GPUs available:\", len(gpus))\n",
    "\tfor gpu in gpus:\n",
    "\t\tprint(gpu)\n",
    "else:\n",
    "\tprint(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from cryptography.hazmat.primitives import hashes, serialization\n",
    "from cryptography.hazmat.primitives.asymmetric import dh\n",
    "from cryptography.hazmat.primitives.kdf.hkdf import HKDF\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from Pyfhel import Pyfhel\n",
    "import nacl.utils\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "from cryptography.hazmat.primitives import padding\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import nacl.utils\n",
    "from nacl.public import PrivateKey, SealedBox\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "\n",
    "# from src.models.FMLEE import FMLEE\n",
    "# from src.data.load_data import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save_dir = \"dataset/mnist_data/\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MAML(tf.keras.Model):\n",
    "\tdef __init__(self, model):\n",
    "\t\tsuper(MAML, self).__init__()\n",
    "\t\tself.model = model\n",
    "\n",
    "\tdef call(self, inputs):\n",
    "\t\tx = tf.reshape(inputs, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "\t\treturn self.model(x)\n",
    "\n",
    "\tdef get_config(self):\n",
    "\t\treturn {\"model\": self.model.get_config()}\n",
    "\n",
    "\t@classmethod\n",
    "\tdef from_config(cls, config):\n",
    "\t\tmodel = tf.keras.models.Model.from_config(config[\"model\"])\n",
    "\t\treturn cls(model)\n",
    "\n",
    "\tdef train_step(self, data):\n",
    "\t\tx, y = data\n",
    "\t\tx = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "\t\ty = tf.reshape(y, (-1,))  # Reshape the target labels\n",
    "\t\twith tf.GradientTape() as tape:\n",
    "\t\t\ty_pred = self.model(x)\n",
    "\t\t\tloss = self.compiled_loss(y, y_pred)\n",
    "\t\tgradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "\t\tself.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\t\tself.compiled_metrics.update_state(y, y_pred)\n",
    "\t\treturn {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\tdef test_step(self, data):\n",
    "\t\tx, y = data\n",
    "\t\tx = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "\t\ty = tf.reshape(y, (-1,))  # Reshape the target labels\n",
    "\t\ty_pred = self.model(x)\n",
    "\t\tself.compiled_loss(y, y_pred)\n",
    "\t\tself.compiled_metrics.update_state(y, y_pred)\n",
    "\t\treturn {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "num_meta_updates = 10\n",
    "num_inner_updates = 5\n",
    "meta_batch_size = 32\n",
    "inner_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMLEE:\n",
    "\tdef __init__(self, no_clients, epochs):\n",
    "\t\tself.no_clients = no_clients\n",
    "\t\tself.epochs = epochs\n",
    "\t\tprint(\"Initializing CKKS scheme...\")\n",
    "\t\tself.HE = self.CKKS()\n",
    "\t\tself.clients = []\n",
    "\t\tprint(\"Initializing clients...\")\n",
    "\t\tself.init_clients()\n",
    "\t\tprint(\"Generating asymmetric keys...\")\n",
    "\t\tself.pvt_key, self.pub_key = self.asym_keygen()\n",
    "\t\tprint(\"Initialization complete.\")\n",
    "\n",
    "\tdef model_spec(self):\n",
    "\t\tmodel = tf.keras.models.Sequential(\n",
    "\t\t\t[\n",
    "\t\t\t\ttf.keras.layers.Conv2D(\n",
    "\t\t\t\t\t32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)\n",
    "\t\t\t\t),\n",
    "\t\t\t\ttf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\t\t\t\ttf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "\t\t\t\ttf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\t\t\t\ttf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "\t\t\t\ttf.keras.layers.Flatten(),\n",
    "\t\t\t\ttf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "\t\t\t\ttf.keras.layers.Dense(10),\n",
    "\t\t\t]\n",
    "\t\t)\n",
    "\t\treturn model\n",
    "\n",
    "\tdef init_model(self):\n",
    "\t\tmodel = MAML(self.model_spec())\n",
    "\t\tmodel.compile(\n",
    "\t\t\toptimizer=tf.keras.optimizers.Adam(),\n",
    "\t\t\tloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "\t\t\tmetrics=[\"accuracy\"],\n",
    "\t\t)\n",
    "\t\treturn model\n",
    "\n",
    "\tdef CKKS(self):\n",
    "\t\tHE = Pyfhel()\n",
    "\t\tckks_params = {\n",
    "\t\t\t\"scheme\": \"CKKS\",\n",
    "\t\t\t\"n\": 2**14,  # Polynomial modulus degree. For CKKS, n/2 values can be\n",
    "\t\t\t\"scale\": 2**30,  # All the encodings will use it for float->fixed point\n",
    "\t\t\t\"qi_sizes\": [\n",
    "\t\t\t\t60,\n",
    "\t\t\t\t30,\n",
    "\t\t\t\t30,\n",
    "\t\t\t\t30,\n",
    "\t\t\t\t60,\n",
    "\t\t\t],\n",
    "\t\t}\n",
    "\t\tprint(\"Generating context for CKKS scheme...\")\n",
    "\t\tHE.contextGen(**ckks_params)  # Generate context for ckks scheme\n",
    "\t\tprint(\"Generating keys for CKKS scheme...\")\n",
    "\t\tHE.keyGen()  # Key Generation: generates a pair of public/secret keys\n",
    "\t\tHE.rotateKeyGen()\n",
    "\t\tHE.relinKeyGen()\n",
    "\t\tprint(\"CKKS scheme initialized.\")\n",
    "\t\treturn HE\n",
    "\n",
    "\tdef asym_keygen(self):\n",
    "\t\tprint(\"Generating private key...\")\n",
    "\t\tpvt_key = PrivateKey.generate()\n",
    "\t\tprint(\"Private key generated.\")\n",
    "\t\tpub_key = pvt_key.public_key\n",
    "\t\tprint(\"Public key generated.\")\n",
    "\t\treturn pvt_key, pub_key\n",
    "\n",
    "\tdef init_clients(self):\n",
    "\t\tfor i in range(self.no_clients):\n",
    "\t\t\tprint(f\"Initializing model for client {i}...\")\n",
    "\t\t\tself.clients.append(self.init_model())\n",
    "\t\t\tprint(f\"Client {i} initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_save_mnist(save_dir):\n",
    "\t(x_train_all, y_train_all), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\t# Save training data with progress bar\n",
    "\tfor array, name in zip(\n",
    "\t\t[x_train_all, y_train_all, x_test, y_test],\n",
    "\t\t[\"x_train.npy\", \"y_train.npy\", \"x_test.npy\", \"y_test.npy\"],\n",
    "\t):\n",
    "\t\twith tqdm(total=len(array), desc=f\"Saving {name}\") as pbar:\n",
    "\t\t\tnp.save(os.path.join(save_dir, name), array)\n",
    "\t\t\tpbar.update(len(array))\n",
    "\n",
    "\tprint(f\"Dataset downloaded and saved locally at {save_dir}\")\n",
    "\n",
    "\n",
    "def load_mnist_from_local(save_dir):\n",
    "\tx_train_all = np.load(os.path.join(save_dir, \"x_train.npy\"))\n",
    "\ty_train_all = np.load(os.path.join(save_dir, \"y_train.npy\"))\n",
    "\tx_test = np.load(os.path.join(save_dir, \"x_test.npy\"))\n",
    "\ty_test = np.load(os.path.join(save_dir, \"y_test.npy\"))\n",
    "\tprint(f\"Dataset loaded from local files at {save_dir}\")\n",
    "\tx_train_all = x_train_all.astype(np.float32) / 255\n",
    "\tx_test = x_test.astype(np.float32) / 255\n",
    "\n",
    "\treturn (x_train_all, y_train_all), (x_test, y_test)\n",
    "\n",
    "\n",
    "def load_mnist():\n",
    "\tif not os.path.exists(os.path.join(save_dir, \"x_train.npy\")):\n",
    "\t\tdownload_and_save_mnist(save_dir)\n",
    "\treturn load_mnist_from_local(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from local files at dataset/mnist_data/\n"
     ]
    }
   ],
   "source": [
    "(x_train_all, y_train_all), (x_test, y_test)  = load_mnist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training and test sets...\n",
      "Data split complete.\n",
      "Training set size: 48000, Temp set size: 12000, Test set size: 10000\n",
      "Splitting temp set into validation and testing sets...\n",
      "Validation and test set split complete.\n",
      "Validation set size: 10200, Test set size: 1800\n",
      "Splitting training data into 3 parts...\n",
      "Part 1 created: 16000 samples.\n",
      "Part 2 created: 16000 samples.\n",
      "Part 3 created: 16000 samples.\n",
      "Data splitting into parts complete.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load MNIST data\n",
    "(x_train_all, y_train_all), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape data\n",
    "x_train_all = x_train_all.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "# Splitting data into training and test sets\n",
    "print(\"Splitting data into training and test sets...\")\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "\tx_train_all, y_train_all, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Data split complete.\")\n",
    "print(\n",
    "\tf\"Training set size: {len(X_train)}, Temp set size: {len(X_temp)}, Test set size: {len(x_test)}\"\n",
    ")\n",
    "\n",
    "# Further split the temporary set into validation and testing sets\n",
    "print(\"Splitting temp set into validation and testing sets...\")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "\tX_temp, y_temp, test_size=0.15, random_state=42\n",
    ")\n",
    "print(\"Validation and test set split complete.\")\n",
    "print(f\"Validation set size: {len(X_val)}, Test set size: {len(X_test)}\")\n",
    "\n",
    "# Split training data into n parts\n",
    "n_parts = no_clients\n",
    "part_size = len(X_train) // n_parts\n",
    "dataset_parts = []\n",
    "\n",
    "print(f\"Splitting training data into {n_parts} parts...\")\n",
    "for i in range(n_parts):\n",
    "\tstart = i * part_size\n",
    "\tend = (i + 1) * part_size if i != n_parts - 1 else len(X_train)\n",
    "\tX_part = X_train[start:end]\n",
    "\ty_part = y_train[start:end]\n",
    "\tdataset_parts.append((X_part, y_part))\n",
    "\tprint(f\"Part {i + 1} created: {len(X_part)} samples.\")\n",
    "\n",
    "print(\"Data splitting into parts complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CKKS scheme...\n",
      "Generating context for CKKS scheme...\n",
      "Generating keys for CKKS scheme...\n",
      "CKKS scheme initialized.\n",
      "Initializing clients...\n",
      "Initializing model for client 0...\n",
      "Client 0 initialized.\n",
      "Initializing model for client 1...\n",
      "Client 1 initialized.\n",
      "Initializing model for client 2...\n",
      "Client 2 initialized.\n",
      "Generating asymmetric keys...\n",
      "Generating private key...\n",
      "Private key generated.\n",
      "Public key generated.\n",
      "Initialization complete.\n"
     ]
    }
   ],
   "source": [
    "fml = FMLEE(no_clients, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<MAML name=maml_9, built=False>,\n",
       " <MAML name=maml_10, built=False>,\n",
       " <MAML name=maml_11, built=False>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fml.clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ckks Pyfhel obj at 0x75d07cdf6da0, [pk:Y, sk:Y, rtk:Y, rlk:Y, contx(n=16384, t=0, sec=128, qi=[60, 30, 30, 30, 60], scale=1073741824.0, )]>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fml.HE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fml.clients[0].fit(x_train_all, y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Pyfhel for CKKS scheme...\n",
      "Generating context for CKKS scheme...\n",
      "Context generation complete.\n",
      "Generating public and secret keys...\n",
      "Public and secret key generation complete.\n",
      "Generating rotation keys...\n",
      "Rotation keys generation complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Pyfhel import Pyfhel\n",
    "\n",
    "\n",
    "def CKKS_keygen():\n",
    "\tprint(\"Initializing Pyfhel for CKKS scheme...\")\n",
    "\tHE = Pyfhel()\n",
    "\n",
    "\tckks_params = {\n",
    "\t\t\"scheme\": \"CKKS\",\n",
    "\t\t\"n\": 2**14,  # Polynomial modulus degree. For CKKS, n/2 values can be\n",
    "\t\t\"scale\": 2**30,  # All the encodings will use it for float->fixed point\n",
    "\t\t\"qi_sizes\": [60, 30, 30, 30, 60],  # Number of bits of each prime in the chain.\n",
    "\t}\n",
    "\n",
    "\tprint(\"Generating context for CKKS scheme...\")\n",
    "\tHE.contextGen(**ckks_params)  # Generate context for ckks scheme\n",
    "\tprint(\"Context generation complete.\")\n",
    "\n",
    "\tprint(\"Generating public and secret keys...\")\n",
    "\tHE.keyGen()  # Key Generation: generates a pair of public/secret keys\n",
    "\tprint(\"Public and secret key generation complete.\")\n",
    "\n",
    "\tprint(\"Generating rotation keys...\")\n",
    "\tHE.rotateKeyGen()\n",
    "\tprint(\"Rotation keys generation complete.\")\n",
    "\n",
    "\treturn HE\n",
    "\n",
    "\n",
    "HE = CKKS_keygen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asym_keygen():\n",
    "\tpvt_key = PrivateKey.generate()\n",
    "\tpub_key = pvt_key.public_key\n",
    "\treturn pvt_key, pub_key\n",
    "\n",
    "\n",
    "agg_pvt_key, agg_pub_key = asym_keygen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nacl_session_keygen():\n",
    "\treturn nacl.utils.random(32)\n",
    "\n",
    "def encrypt_symmetric_key(pub_key, symmetric_key):\n",
    "\tsealed_box = SealedBox(pub_key)\n",
    "\treturn sealed_box.encrypt(symmetric_key)\n",
    "\n",
    "def decrypt_symmetric_key(pvt_key, encrypted_key):\n",
    "\tsealed_box = SealedBox(pvt_key)\n",
    "\treturn sealed_box.decrypt(encrypted_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def maml_train_step(model, x_train, y_train, inner_lr, num_inner_updates):\n",
    "\n",
    "\tmodel.fit(\n",
    "\t\t\tx_train,\n",
    "\t\t\ty_train,\n",
    "\t\t\tepochs=1,\n",
    "\t\t\tbatch_size=64,\n",
    "\t\t\tverbose=1,\n",
    "\t\t\tvalidation_data=(x_train, y_train),\n",
    "\t\t)\n",
    "\twith tf.GradientTape() as outer_tape:\n",
    "\t\tfor i in range(num_inner_updates):\n",
    "\t\t\twith tf.GradientTape() as inner_tape:\n",
    "\t\t\t\tpredictions = model(x_train, training=True)\n",
    "\t\t\t\tloss = tf.reduce_mean(\n",
    "\t\t\t\t\ttf.keras.losses.sparse_categorical_crossentropy(\n",
    "\t\t\t\t\t\ty_train, predictions\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t)\n",
    "\t\t\tgrads = inner_tape.gradient(loss, model.trainable_variables)\n",
    "\t\t\tfor var, grad in zip(model.trainable_variables, grads):\n",
    "\t\t\t\tif grad is not None:\n",
    "\t\t\t\t\tvar.assign_sub(inner_lr * grad)\n",
    "\n",
    "\t\tpredictions = model(x_train, training=True)\n",
    "\t\touter_loss = tf.reduce_mean(\n",
    "\t\t\ttf.keras.losses.sparse_categorical_crossentropy(y_train, predictions)\n",
    "\t\t)\n",
    "\n",
    "\touter_grads = outer_tape.gradient(outer_loss, model.trainable_variables)\n",
    "\treturn outer_loss, outer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HE_encrypt(wtarray):\n",
    "\tcwt = []\n",
    "\tfor layer in wtarray:\n",
    "\t\tflat_array = layer.astype(np.float64).flatten()\n",
    "\n",
    "\t\tchunks = np.array_split(flat_array, (len(flat_array) + 2**10 - 1) // 2**10)\n",
    "\t\tclayer = []\n",
    "\t\t\n",
    "\t\tfor chunk in chunks:\n",
    "\t\t\tptxt = HE.encodeFrac(chunk)\n",
    "\t\t\tctxt = HE.encryptPtxt(ptxt)\n",
    "\t\t\tclayer.append(ctxt)\n",
    "\t\tcwt.append(clayer.copy())\n",
    "\t\t\n",
    "\treturn cwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_message_sym_AES(key, message):\n",
    "\tserialized_obj = pickle.dumps(message)\n",
    "\n",
    "\tiv = nacl.utils.random(16)\n",
    "\n",
    "\tpadder = padding.PKCS7(algorithms.AES.block_size).padder()\n",
    "\tpadded_obj = padder.update(serialized_obj) + padder.finalize()\n",
    "\n",
    "\tcipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n",
    "\tencryptor = cipher.encryptor()\n",
    "\tciphertext = encryptor.update(padded_obj) + encryptor.finalize()\n",
    "\n",
    "\treturn iv + ciphertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_message_sym_AES(key , ciphertext):\n",
    "\tiv = ciphertext[:16]\n",
    "\tciphertext = ciphertext[16:]\n",
    "\n",
    "\tcipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n",
    "\tdecryptor = cipher.decryptor()\n",
    "\tpadded_obj = decryptor.update(ciphertext) + decryptor.finalize()\n",
    "\n",
    "\tunpadder = padding.PKCS7(algorithms.AES.block_size).unpadder()\n",
    "\tunpadded_obj = unpadder.update(padded_obj) + unpadder.finalize()\n",
    "\n",
    "\treturn pickle.loads(unpadded_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wts_ckks(encrypted_wts):\n",
    "\tres_wts = []\n",
    "\tprint(len(encrypted_wts) )\n",
    "\tprint(len(encrypted_wts[0]) )\n",
    "\tfor j in range(len(encrypted_wts[0])):\n",
    "\t\tlayer = []\n",
    "\t\tfor k in range(len(encrypted_wts[0][j])):\n",
    "\t\t\ttmp = encrypted_wts[0][j][k].copy()\n",
    "\n",
    "\t\t\tfor i in range(1, len(encrypted_wts)):\n",
    "\t\t\t\ttmp = tmp + encrypted_wts[i][j][k]\n",
    "\n",
    "\t\t\ttmp = tmp / len(encrypted_wts)\n",
    "\t\t\tlayer.append(tmp)\n",
    "\n",
    "\t\tres_wts.append(layer.copy())\n",
    "\t\t\n",
    "\treturn res_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_wts_ckks(encrypted_wts):\n",
    "\tdecrypted_wts = []\n",
    "\twtarray = dummy_model.get_weights()\n",
    "\n",
    "\tfor layer_wts , layer in zip(encrypted_wts , wtarray):\n",
    "\t\tdecrypted_layer = []\n",
    "\t\tflat_array = layer.astype(np.float64).flatten()\n",
    "\t\tchunks = np.array_split(flat_array, (len(flat_array) + 2**13 - 1) // 2**13)\n",
    "\n",
    "\t\tfor chunk , cchunk in zip(chunks , layer_wts):\n",
    "\t\t\tdecrypted_chunk = HE.decryptFrac(cchunk)\n",
    "\t\t\toriginal_chunk_size  = len(chunk)\n",
    "\t\t\tdecrypted_chunk = decrypted_chunk[:original_chunk_size]\n",
    "\t\t\t\n",
    "\t\t\tdecrypted_layer.append(decrypted_chunk)\n",
    "\t\t\t\n",
    "\t\tdecrypted_layer = np.concatenate(decrypted_layer)\n",
    "\t\tdecrypted_layer = decrypted_layer.reshape(layer.shape)\n",
    "\t\tdecrypted_wts.append(decrypted_layer)\n",
    "\t\t\n",
    "\treturn decrypted_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wts(wts):\n",
    "\tpeeled_wts = []\n",
    "\tfor client_id in range(no_clients):\n",
    "\t\tsesion_key = decrypt_symmetric_key(agg_pvt_key , agg_sesion_keys[client_id])\n",
    "\t\tpeeled_wt = decrypt_message_sym_AES(sesion_key, wts[client_id])\n",
    "\t\tpeeled_wts.append(peeled_wt)\n",
    "\tres_wts = aggregate_wts_ckks(peeled_wts)\n",
    "\treturn res_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = fml.clients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_sesion_keys = [0 for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_lr = 0.001\n",
    "num_inner_updates = 1\n",
    "outer_lr = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [[] for i in range(no_clients)]\n",
    "losses = [[] for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_session_keys = [0 for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_wts = [0 for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.7395 - loss: -0.0288 - val_accuracy: 0.9573 - val_loss: -0.0958\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.7445 - loss: -0.2572 - val_accuracy: 0.9545 - val_loss: -0.6079\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.7172 - loss: 0.7054 - val_accuracy: 0.9564 - val_loss: 0.3457\n",
      "101725776\n",
      "3\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:49<01:38, 49.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.5957 - loss: -1.0453 - val_accuracy: 0.9247 - val_loss: -1.8873\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.5735 - loss: -1.2432 - val_accuracy: 0.9319 - val_loss: -2.1239\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.6469 - loss: -0.7827 - val_accuracy: 0.9199 - val_loss: -1.3466\n",
      "101725776\n",
      "3\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [01:35<00:47, 47.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6171 - loss: -1.0251 - val_accuracy: 0.9433 - val_loss: -2.2790\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6505 - loss: -1.3621 - val_accuracy: 0.9430 - val_loss: -2.4384\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6086 - loss: -0.8146 - val_accuracy: 0.9412 - val_loss: -1.5550\n",
      "101725776\n",
      "3\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [02:22<00:00, 47.35s/it]\n"
     ]
    }
   ],
   "source": [
    "for r in tqdm(range(epochs)):\n",
    "\tfor client_id , (client , client_dataset) in enumerate(zip(fml.clients , dataset_parts)):\n",
    "\t\tmodel = client\n",
    "\t\tx_train, y_train = client_dataset\n",
    "\n",
    "\t\touter_loss, outer_grads = maml_train_step(\n",
    "\t\t\t\tmodel, x_train, y_train, inner_lr, num_inner_updates\n",
    "\t\t\t)\n",
    "\t\toptimizer = tf.keras.optimizers.Adam(learning_rate=outer_lr)\n",
    "\t\toptimizer.apply_gradients(zip(outer_grads, model.trainable_variables))\n",
    "\t\thistory = model.evaluate(\n",
    "\t\t\tx_train,\n",
    "\t\t\ty_train,\n",
    "\t\t\tbatch_size=64,\n",
    "\t\t\tverbose=0,\n",
    "\t\t)\n",
    "\t\taccuracies[client_id].append(history[1])\n",
    "\t\tlosses[client_id].append(history[0])\n",
    "\t\ttrained_weights = model.get_weights()\n",
    "\n",
    "\t\tsession_key = nacl_session_keygen()\n",
    "\t\tclient_session_keys[client_id] = session_key\n",
    "\t\tenc_session_key = encrypt_symmetric_key(agg_pub_key , session_key)\n",
    "\t\tagg_sesion_keys[client_id] = enc_session_key\n",
    "\n",
    "\t\tHe_ciphertext = HE_encrypt(trained_weights)\n",
    "\t\tsym_ctxt = encrypt_message_sym_AES(session_key , He_ciphertext)\n",
    "\t\tenc_wts[client_id] = sym_ctxt\n",
    "\tprint(len(enc_wts[0]))\n",
    "\tagg_wts = aggregate_wts(enc_wts)\n",
    "\n",
    "\tfor client_id , client in enumerate(fml.clients):\n",
    "\t\tnew_wts = decrypt_wts_ckks(agg_wts)        \n",
    "\n",
    "\t\tclient.set_weights(new_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.116286277770996, 0.9089999794960022]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_parts[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmlee_accuracies = accuracies\n",
    "fmlee_losses = losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CKKS scheme...\n",
      "Generating context for CKKS scheme...\n",
      "Generating keys for CKKS scheme...\n",
      "CKKS scheme initialized.\n",
      "Initializing clients...\n",
      "Initializing model for client 0...\n",
      "Client 0 initialized.\n",
      "Initializing model for client 1...\n",
      "Client 1 initialized.\n",
      "Initializing model for client 2...\n",
      "Client 2 initialized.\n",
      "Generating asymmetric keys...\n",
      "Generating private key...\n",
      "Private key generated.\n",
      "Public key generated.\n",
      "Initialization complete.\n"
     ]
    }
   ],
   "source": [
    "no_fml = FMLEE(no_clients, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [[] for i in range(no_clients)]\n",
    "losses = [[] for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wts_noenc(weight_list):\n",
    "    n_models = len(weight_list)\n",
    "    # Initialize the aggregated weights with zeros of the same shape as the first model's weights\n",
    "    aggregated_weights = [np.zeros_like(w) for w in weight_list[0]]\n",
    "\n",
    "    # Sum the weights for each layer\n",
    "    for weights in weight_list:\n",
    "        for i, weight in enumerate(weights):\n",
    "            aggregated_weights[i] += weight\n",
    "\n",
    "    # Divide by the number of models to get the average\n",
    "    for i in range(len(aggregated_weights)):\n",
    "        aggregated_weights[i] /= n_models\n",
    "\n",
    "    return aggregated_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6005 - loss: -1.4667 - val_accuracy: 0.9352 - val_loss: -2.9217\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.5982 - loss: -0.9547 - val_accuracy: 0.9500 - val_loss: -1.8954\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.5711 - loss: -1.2228 - val_accuracy: 0.9467 - val_loss: -2.3903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:41<01:22, 41.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9283 - loss: -2.4199 - val_accuracy: 0.9663 - val_loss: -2.4230\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9295 - loss: -2.3908 - val_accuracy: 0.9699 - val_loss: -2.3831\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9275 - loss: -2.5641 - val_accuracy: 0.9649 - val_loss: -2.6319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [01:21<00:40, 40.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9654 - loss: -3.0352 - val_accuracy: 0.9770 - val_loss: -2.9188\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9643 - loss: -2.9149 - val_accuracy: 0.9754 - val_loss: -2.8710\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9668 - loss: -3.0407 - val_accuracy: 0.9749 - val_loss: -2.8254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [02:02<00:00, 40.79s/it]\n"
     ]
    }
   ],
   "source": [
    "for r in tqdm(range(epochs)):\n",
    "    for client_id, (client, client_dataset) in enumerate(\n",
    "        zip(fml.clients, dataset_parts)\n",
    "    ):\n",
    "        model = client\n",
    "        x_train, y_train = client_dataset\n",
    "\n",
    "        outer_loss, outer_grads = maml_train_step(\n",
    "            model, x_train, y_train, inner_lr, num_inner_updates\n",
    "        )\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=outer_lr)\n",
    "        optimizer.apply_gradients(zip(outer_grads, model.trainable_variables))\n",
    "        history = model.evaluate(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            batch_size=64,\n",
    "            verbose=0,\n",
    "        )\n",
    "        accuracies[client_id].append(history[1])\n",
    "        losses[client_id].append(history[0])\n",
    "        trained_weights = model.get_weights()\n",
    "\n",
    "        enc_wts[client_id] = trained_weights\n",
    "    agg_wts = aggregate_wts_noenc(enc_wts)\n",
    "\n",
    "    for client_id, client in enumerate(fml.clients):\n",
    "        client.set_weights(agg_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m924s\u001b[0m 3s/step - accuracy: 0.0988 - loss: 2.3847\n",
      "{'accuracy': [0.09839999675750732], 'loss': [2.3259079456329346]}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "num_samples = 100\n",
    "num_classes = 10\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# Generate random images and labels\n",
    "x_dummy = np.random.rand(num_samples, *input_shape).astype(np.float32)\n",
    "y_dummy = np.random.randint(0, num_classes, num_samples)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_dummy = tf.keras.utils.to_categorical(y_dummy, num_classes)\n",
    "\n",
    "# Load MobileNetV2 with pre-trained weights and exclude the top layers\n",
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Add custom top layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Create the full model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model for one epoch\n",
    "history = model.fit(x_dummy, y_dummy, epochs=1, batch_size=32)\n",
    "\n",
    "# Print the training history\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1226/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1:02\u001b[0m 3s/step - accuracy: 0.5000 - loss: 0.6931"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Train the model for one epoch\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dummy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_dummy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Print the training history\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory)\n",
      "File \u001b[0;32m~/miniconda3/envs/f39/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/f39/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/miniconda3/envs/f39/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/f39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/f39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/f39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/f39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/f39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/f39/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/f39/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/f39/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    UpSampling2D,\n",
    "    Concatenate,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Define U-Net architecture\n",
    "def unet(input_size=(128, 128, 1)):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    conv1 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "    conv3 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n",
    "    conv4 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(1024, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n",
    "    conv5 = Conv2D(1024, (3, 3), activation=\"relu\", padding=\"same\")(conv5)\n",
    "\n",
    "    # Decoder\n",
    "    up6 = UpSampling2D(size=(2, 2))(conv5)\n",
    "    up6 = Conv2D(512, (2, 2), activation=\"relu\", padding=\"same\")(up6)\n",
    "    merge6 = Concatenate()([conv4, up6])\n",
    "    conv6 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(merge6)\n",
    "    conv6 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(conv6)\n",
    "\n",
    "    up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    up7 = Conv2D(256, (2, 2), activation=\"relu\", padding=\"same\")(up7)\n",
    "    merge7 = Concatenate()([conv3, up7])\n",
    "    conv7 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(merge7)\n",
    "    conv7 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(conv7)\n",
    "\n",
    "    up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    up8 = Conv2D(128, (2, 2), activation=\"relu\", padding=\"same\")(up8)\n",
    "    merge8 = Concatenate()([conv2, up8])\n",
    "    conv8 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(merge8)\n",
    "    conv8 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(conv8)\n",
    "\n",
    "    up9 = UpSampling2D(size=(2, 2))(conv8)\n",
    "    up9 = Conv2D(64, (2, 2), activation=\"relu\", padding=\"same\")(up9)\n",
    "    merge9 = Concatenate()([conv1, up9])\n",
    "    conv9 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(merge9)\n",
    "    conv9 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(conv9)\n",
    "    conv9 = Conv2D(2, (3, 3), activation=\"relu\", padding=\"same\")(conv9)\n",
    "    conv10 = Conv2D(1, (1, 1), activation=\"sigmoid\")(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a dummy dataset\n",
    "num_samples = 100\n",
    "input_shape = (128, 128, 1)\n",
    "\n",
    "# Generate random images and labels\n",
    "x_dummy = np.random.rand(num_samples, *input_shape).astype(np.float32)\n",
    "y_dummy = np.random.randint(0, 2, (num_samples, 128, 128, 1)).astype(np.float32)\n",
    "\n",
    "unet_models = []\n",
    "for _ in range(len(clients)):\n",
    "# Instantiate the U-Net model\n",
    "    model = unet(input_shape)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    unet_models.append(model)\n",
    "# Train the model for one epoch\n",
    "history = model.fit(x_dummy, y_dummy, epochs=1, batch_size=8)\n",
    "\n",
    "# Print the training history\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in tqdm(range(1)):\n",
    "    for client_id, (unet_models, client_dataset) in enumerate(\n",
    "        zip(clients, dataset_parts)\n",
    "    ):\n",
    "        model = client\n",
    "        x_train, y_train = client_dataset\n",
    "\n",
    "        outer_loss, outer_grads = maml_train_step(\n",
    "            model, x_train, y_train, inner_lr, num_inner_updates\n",
    "        )\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=outer_lr)\n",
    "        optimizer.apply_gradients(zip(outer_grads, model.trainable_variables))\n",
    "        history = model.fit(x_dummy, y_dummy, epochs=1, batch_size=8)\n",
    "        \n",
    "        trained_weights = model.get_weights()\n",
    "        resnet_enc_st = tm.time()\n",
    "        \n",
    "        session_key = nacl_session_keygen()\n",
    "        client_session_keys[client_id] = session_key\n",
    "        enc_session_key = encrypt_symmetric_key(agg_pub_key, session_key)\n",
    "        agg_sesion_keys[client_id] = enc_session_key\n",
    "\n",
    "        He_ciphertext = HE_encrypt(trained_weights)\n",
    "        sym_ctxt = encrypt_message_sym_AES(session_key, He_ciphertext)\n",
    "        enc_wts[client_id] = sym_ctxt\n",
    "        resnet_enc_stop = tm.time()\n",
    "    print(len(enc_wts[0]))\n",
    "    \n",
    "    resnet_agg_st = tm.time()\n",
    "    agg_wts = aggregate_wts(enc_wts)\n",
    "    resnet_agg_stop = tm.time()\n",
    "    \n",
    "    for client_id, client in enumerate(fml.clients):\n",
    "        resnet_dec_st = tm.time()\n",
    "        new_wts = decrypt_wts_ckks(agg_wts)\n",
    "        resnet_dec_stop = tm.time()\n",
    "        client.set_weights(new_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Create a dummy dataset\n",
    "num_samples = 100\n",
    "num_classes = 10\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# Generate random images and labels\n",
    "x_dummy = np.random.rand(num_samples, *input_shape).astype(np.float32)\n",
    "y_dummy = np.random.randint(0, num_classes, num_samples)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_dummy = tf.keras.utils.to_categorical(y_dummy, num_classes)\n",
    "\n",
    "# Load ResNet50 with pre-trained weights and exclude the top layers\n",
    "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Add custom top layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "renet_models = []\n",
    "for _ in range(len(clients)):\n",
    "    # Create the full model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    resnet_models.append(model)\n",
    "    # \n",
    "# Create the full model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model for one epoch\n",
    "history = model.fit(x_dummy, y_dummy, epochs=1, batch_size=32)\n",
    "\n",
    "# Print the training history\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in tqdm(range(1)):\n",
    "    for client_id, (resnet_models, client_dataset) in enumerate(\n",
    "        zip(clients, dataset_parts)\n",
    "    ):\n",
    "        model = client\n",
    "        x_train, y_train = client_dataset\n",
    "\n",
    "        outer_loss, outer_grads = maml_train_step(\n",
    "            model, x_train, y_train, inner_lr, num_inner_updates\n",
    "        )\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=outer_lr)\n",
    "        optimizer.apply_gradients(zip(outer_grads, model.trainable_variables))\n",
    "        history = model.fit(x_dummy, y_dummy, epochs=1, batch_size=8)\n",
    "\n",
    "        trained_weights = model.get_weights()\n",
    "        unet_enc_st = tm.time()\n",
    "\n",
    "        session_key = nacl_session_keygen()\n",
    "        client_session_keys[client_id] = session_key\n",
    "        enc_session_key = encrypt_symmetric_key(agg_pub_key, session_key)\n",
    "        agg_sesion_keys[client_id] = enc_session_key\n",
    "\n",
    "        He_ciphertext = HE_encrypt(trained_weights)\n",
    "        sym_ctxt = encrypt_message_sym_AES(session_key, He_ciphertext)\n",
    "        enc_wts[client_id] = sym_ctxt\n",
    "        unet_enc_stop = tm.time()\n",
    "    print(len(enc_wts[0]))\n",
    "\n",
    "    unet_agg_st = tm.time()\n",
    "    agg_wts = aggregate_wts(enc_wts)\n",
    "    unet_agg_stop = tm.time()\n",
    "\n",
    "    for client_id, client in enumerate(fml.clients):\n",
    "        unet_dec_st = tm.time()\n",
    "        new_wts = decrypt_wts_ckks(agg_wts)\n",
    "        resnet_dec_stop = tm.time()\n",
    "        client.set_weights(new_wts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "he39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
