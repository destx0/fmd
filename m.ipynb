{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pyfhel in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.21 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from Pyfhel) (1.26.4)\n",
      "Requirement already satisfied: pynacl in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (1.5.0)\n",
      "Requirement already satisfied: cffi>=1.4.1 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from pynacl) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from cffi>=1.4.1->pynacl) (2.22)\n",
      "Requirement already satisfied: cryptography in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (42.0.8)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from cryptography) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from cffi>=1.12->cryptography) (2.22)\n",
      "Requirement already satisfied: tqdm in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (4.66.4)\n",
      "Requirement already satisfied: scikit-learn in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Pyfhel\n",
    "!pip install pynacl\n",
    "!pip install cryptography\n",
    "!pip install tqdm\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_clients = 3\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 13:50:23.843427: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-19 13:50:24.060924: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-19 13:50:24.795223: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 13:50:27.832817: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "No GPU available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 13:50:30.284223: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 13:50:30.284839: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Check if GPU is available\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "\tprint(\"GPUs available:\", len(gpus))\n",
    "\tfor gpu in gpus:\n",
    "\t\tprint(gpu)\n",
    "else:\n",
    "\tprint(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from cryptography.hazmat.primitives import hashes, serialization\n",
    "from cryptography.hazmat.primitives.asymmetric import dh\n",
    "from cryptography.hazmat.primitives.kdf.hkdf import HKDF\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from Pyfhel import Pyfhel\n",
    "import nacl.utils\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "from cryptography.hazmat.primitives import padding\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import nacl.utils\n",
    "from nacl.public import PrivateKey, SealedBox\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "\n",
    "# from src.models.FMLEE import FMLEE\n",
    "# from src.data.load_data import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save_dir = \"dataset/mnist_data/\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MAML(tf.keras.Model):\n",
    "\tdef __init__(self, model):\n",
    "\t\tsuper(MAML, self).__init__()\n",
    "\t\tself.model = model\n",
    "\n",
    "\tdef call(self, inputs):\n",
    "\t\tx = tf.reshape(inputs, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "\t\treturn self.model(x)\n",
    "\n",
    "\tdef get_config(self):\n",
    "\t\treturn {\"model\": self.model.get_config()}\n",
    "\n",
    "\t@classmethod\n",
    "\tdef from_config(cls, config):\n",
    "\t\tmodel = tf.keras.models.Model.from_config(config[\"model\"])\n",
    "\t\treturn cls(model)\n",
    "\n",
    "\tdef train_step(self, data):\n",
    "\t\tx, y = data\n",
    "\t\tx = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "\t\ty = tf.reshape(y, (-1,))  # Reshape the target labels\n",
    "\t\twith tf.GradientTape() as tape:\n",
    "\t\t\ty_pred = self.model(x)\n",
    "\t\t\tloss = self.compiled_loss(y, y_pred)\n",
    "\t\tgradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "\t\tself.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\t\tself.compiled_metrics.update_state(y, y_pred)\n",
    "\t\treturn {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\tdef test_step(self, data):\n",
    "\t\tx, y = data\n",
    "\t\tx = tf.reshape(x, (-1, 28, 28, 1))  # Reshape the input tensor\n",
    "\t\ty = tf.reshape(y, (-1,))  # Reshape the target labels\n",
    "\t\ty_pred = self.model(x)\n",
    "\t\tself.compiled_loss(y, y_pred)\n",
    "\t\tself.compiled_metrics.update_state(y, y_pred)\n",
    "\t\treturn {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "num_meta_updates = 10\n",
    "num_inner_updates = 5\n",
    "meta_batch_size = 32\n",
    "inner_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMLEE:\n",
    "\tdef __init__(self, no_clients, epochs):\n",
    "\t\tself.no_clients = no_clients\n",
    "\t\tself.epochs = epochs\n",
    "\t\tprint(\"Initializing CKKS scheme...\")\n",
    "\t\tself.HE = self.CKKS()\n",
    "\t\tself.clients = []\n",
    "\t\tprint(\"Initializing clients...\")\n",
    "\t\tself.init_clients()\n",
    "\t\tprint(\"Generating asymmetric keys...\")\n",
    "\t\tself.pvt_key, self.pub_key = self.asym_keygen()\n",
    "\t\tprint(\"Initialization complete.\")\n",
    "\n",
    "\tdef model_spec(self):\n",
    "\t\tmodel = tf.keras.models.Sequential(\n",
    "\t\t\t[\n",
    "\t\t\t\ttf.keras.layers.Conv2D(\n",
    "\t\t\t\t\t32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)\n",
    "\t\t\t\t),\n",
    "\t\t\t\ttf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\t\t\t\ttf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "\t\t\t\ttf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\t\t\t\ttf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "\t\t\t\ttf.keras.layers.Flatten(),\n",
    "\t\t\t\ttf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "\t\t\t\ttf.keras.layers.Dense(10),\n",
    "\t\t\t]\n",
    "\t\t)\n",
    "\t\treturn model\n",
    "\n",
    "\tdef init_model(self):\n",
    "\t\tmodel = MAML(self.model_spec())\n",
    "\t\tmodel.compile(\n",
    "\t\t\toptimizer=tf.keras.optimizers.Adam(),\n",
    "\t\t\tloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "\t\t\tmetrics=[\"accuracy\"],\n",
    "\t\t)\n",
    "\t\treturn model\n",
    "\n",
    "\tdef CKKS(self):\n",
    "\t\tHE = Pyfhel()\n",
    "\t\tckks_params = {\n",
    "\t\t\t\"scheme\": \"CKKS\",\n",
    "\t\t\t\"n\": 2**14,  # Polynomial modulus degree. For CKKS, n/2 values can be\n",
    "\t\t\t\"scale\": 2**30,  # All the encodings will use it for float->fixed point\n",
    "\t\t\t\"qi_sizes\": [\n",
    "\t\t\t\t60,\n",
    "\t\t\t\t30,\n",
    "\t\t\t\t30,\n",
    "\t\t\t\t30,\n",
    "\t\t\t\t60,\n",
    "\t\t\t],\n",
    "\t\t}\n",
    "\t\tprint(\"Generating context for CKKS scheme...\")\n",
    "\t\tHE.contextGen(**ckks_params)  # Generate context for ckks scheme\n",
    "\t\tprint(\"Generating keys for CKKS scheme...\")\n",
    "\t\tHE.keyGen()  # Key Generation: generates a pair of public/secret keys\n",
    "\t\tHE.rotateKeyGen()\n",
    "\t\tHE.relinKeyGen()\n",
    "\t\tprint(\"CKKS scheme initialized.\")\n",
    "\t\treturn HE\n",
    "\n",
    "\tdef asym_keygen(self):\n",
    "\t\tprint(\"Generating private key...\")\n",
    "\t\tpvt_key = PrivateKey.generate()\n",
    "\t\tprint(\"Private key generated.\")\n",
    "\t\tpub_key = pvt_key.public_key\n",
    "\t\tprint(\"Public key generated.\")\n",
    "\t\treturn pvt_key, pub_key\n",
    "\n",
    "\tdef init_clients(self):\n",
    "\t\tfor i in range(self.no_clients):\n",
    "\t\t\tprint(f\"Initializing model for client {i}...\")\n",
    "\t\t\tself.clients.append(self.init_model())\n",
    "\t\t\tprint(f\"Client {i} initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_save_mnist(save_dir):\n",
    "\t(x_train_all, y_train_all), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\t# Save training data with progress bar\n",
    "\tfor array, name in zip(\n",
    "\t\t[x_train_all, y_train_all, x_test, y_test],\n",
    "\t\t[\"x_train.npy\", \"y_train.npy\", \"x_test.npy\", \"y_test.npy\"],\n",
    "\t):\n",
    "\t\twith tqdm(total=len(array), desc=f\"Saving {name}\") as pbar:\n",
    "\t\t\tnp.save(os.path.join(save_dir, name), array)\n",
    "\t\t\tpbar.update(len(array))\n",
    "\n",
    "\tprint(f\"Dataset downloaded and saved locally at {save_dir}\")\n",
    "\n",
    "\n",
    "def load_mnist_from_local(save_dir):\n",
    "\tx_train_all = np.load(os.path.join(save_dir, \"x_train.npy\"))\n",
    "\ty_train_all = np.load(os.path.join(save_dir, \"y_train.npy\"))\n",
    "\tx_test = np.load(os.path.join(save_dir, \"x_test.npy\"))\n",
    "\ty_test = np.load(os.path.join(save_dir, \"y_test.npy\"))\n",
    "\tprint(f\"Dataset loaded from local files at {save_dir}\")\n",
    "\tx_train_all = x_train_all.astype(np.float32) / 255\n",
    "\tx_test = x_test.astype(np.float32) / 255\n",
    "\n",
    "\treturn (x_train_all, y_train_all), (x_test, y_test)\n",
    "\n",
    "\n",
    "def load_mnist():\n",
    "\tif not os.path.exists(os.path.join(save_dir, \"x_train.npy\")):\n",
    "\t\tdownload_and_save_mnist(save_dir)\n",
    "\treturn load_mnist_from_local(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from local files at dataset/mnist_data/\n"
     ]
    }
   ],
   "source": [
    "(x_train_all, y_train_all), (x_test, y_test)  = load_mnist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training and test sets...\n",
      "Data split complete.\n",
      "Training set size: 48000, Temp set size: 12000, Test set size: 10000\n",
      "Splitting temp set into validation and testing sets...\n",
      "Validation and test set split complete.\n",
      "Validation set size: 10200, Test set size: 1800\n",
      "Splitting training data into 3 parts...\n",
      "Part 1 created: 16000 samples.\n",
      "Part 2 created: 16000 samples.\n",
      "Part 3 created: 16000 samples.\n",
      "Data splitting into parts complete.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load MNIST data\n",
    "(x_train_all, y_train_all), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape data\n",
    "x_train_all = x_train_all.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "# Splitting data into training and test sets\n",
    "print(\"Splitting data into training and test sets...\")\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "\tx_train_all, y_train_all, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Data split complete.\")\n",
    "print(\n",
    "\tf\"Training set size: {len(X_train)}, Temp set size: {len(X_temp)}, Test set size: {len(x_test)}\"\n",
    ")\n",
    "\n",
    "# Further split the temporary set into validation and testing sets\n",
    "print(\"Splitting temp set into validation and testing sets...\")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "\tX_temp, y_temp, test_size=0.15, random_state=42\n",
    ")\n",
    "print(\"Validation and test set split complete.\")\n",
    "print(f\"Validation set size: {len(X_val)}, Test set size: {len(X_test)}\")\n",
    "\n",
    "# Split training data into n parts\n",
    "n_parts = no_clients\n",
    "part_size = len(X_train) // n_parts\n",
    "dataset_parts = []\n",
    "\n",
    "print(f\"Splitting training data into {n_parts} parts...\")\n",
    "for i in range(n_parts):\n",
    "\tstart = i * part_size\n",
    "\tend = (i + 1) * part_size if i != n_parts - 1 else len(X_train)\n",
    "\tX_part = X_train[start:end]\n",
    "\ty_part = y_train[start:end]\n",
    "\tdataset_parts.append((X_part, y_part))\n",
    "\tprint(f\"Part {i + 1} created: {len(X_part)} samples.\")\n",
    "\n",
    "print(\"Data splitting into parts complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CKKS scheme...\n",
      "Generating context for CKKS scheme...\n",
      "Generating keys for CKKS scheme...\n",
      "CKKS scheme initialized.\n",
      "Initializing clients...\n",
      "Initializing model for client 0...\n",
      "Client 0 initialized.\n",
      "Initializing model for client 1...\n",
      "Client 1 initialized.\n",
      "Initializing model for client 2...\n",
      "Client 2 initialized.\n",
      "Generating asymmetric keys...\n",
      "Generating private key...\n",
      "Private key generated.\n",
      "Public key generated.\n",
      "Initialization complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "fml = FMLEE(no_clients, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<MAML name=maml, built=False>,\n",
       " <MAML name=maml_1, built=False>,\n",
       " <MAML name=maml_2, built=False>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fml.clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ckks Pyfhel obj at 0x76c77ea903a0, [pk:Y, sk:Y, rtk:Y, rlk:Y, contx(n=16384, t=0, sec=128, qi=[60, 30, 30, 30, 60], scale=1073741824.0, )]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fml.HE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fml.clients[0].fit(x_train_all, y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Pyfhel for CKKS scheme...\n",
      "Generating context for CKKS scheme...\n",
      "Context generation complete.\n",
      "Generating public and secret keys...\n",
      "Public and secret key generation complete.\n",
      "Generating rotation keys...\n",
      "Rotation keys generation complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Pyfhel import Pyfhel\n",
    "\n",
    "\n",
    "def CKKS_keygen():\n",
    "\tprint(\"Initializing Pyfhel for CKKS scheme...\")\n",
    "\tHE = Pyfhel()\n",
    "\n",
    "\tckks_params = {\n",
    "\t\t\"scheme\": \"CKKS\",\n",
    "\t\t\"n\": 2**14,  # Polynomial modulus degree. For CKKS, n/2 values can be\n",
    "\t\t\"scale\": 2**30,  # All the encodings will use it for float->fixed point\n",
    "\t\t\"qi_sizes\": [60, 30, 30, 30, 60],  # Number of bits of each prime in the chain.\n",
    "\t}\n",
    "\n",
    "\tprint(\"Generating context for CKKS scheme...\")\n",
    "\tHE.contextGen(**ckks_params)  # Generate context for ckks scheme\n",
    "\tprint(\"Context generation complete.\")\n",
    "\n",
    "\tprint(\"Generating public and secret keys...\")\n",
    "\tHE.keyGen()  # Key Generation: generates a pair of public/secret keys\n",
    "\tprint(\"Public and secret key generation complete.\")\n",
    "\n",
    "\tprint(\"Generating rotation keys...\")\n",
    "\tHE.rotateKeyGen()\n",
    "\tprint(\"Rotation keys generation complete.\")\n",
    "\n",
    "\treturn HE\n",
    "\n",
    "\n",
    "HE = CKKS_keygen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asym_keygen():\n",
    "\tpvt_key = PrivateKey.generate()\n",
    "\tpub_key = pvt_key.public_key\n",
    "\treturn pvt_key, pub_key\n",
    "\n",
    "\n",
    "agg_pvt_key, agg_pub_key = asym_keygen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nacl_session_keygen():\n",
    "\treturn nacl.utils.random(32)\n",
    "\n",
    "def encrypt_symmetric_key(pub_key, symmetric_key):\n",
    "\tsealed_box = SealedBox(pub_key)\n",
    "\treturn sealed_box.encrypt(symmetric_key)\n",
    "\n",
    "def decrypt_symmetric_key(pvt_key, encrypted_key):\n",
    "\tsealed_box = SealedBox(pvt_key)\n",
    "\treturn sealed_box.decrypt(encrypted_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def maml_train_step(model, x_train, y_train, inner_lr, num_inner_updates):\n",
    "\n",
    "\tmodel.fit(\n",
    "\t\t\tx_train,\n",
    "\t\t\ty_train,\n",
    "\t\t\tepochs=1,\n",
    "\t\t\tbatch_size=64,\n",
    "\t\t\tverbose=1,\n",
    "\t\t\tvalidation_data=(x_train, y_train),\n",
    "\t\t)\n",
    "\twith tf.GradientTape() as outer_tape:\n",
    "\t\tfor i in range(num_inner_updates):\n",
    "\t\t\twith tf.GradientTape() as inner_tape:\n",
    "\t\t\t\tpredictions = model(x_train, training=True)\n",
    "\t\t\t\tloss = tf.reduce_mean(\n",
    "\t\t\t\t\ttf.keras.losses.sparse_categorical_crossentropy(\n",
    "\t\t\t\t\t\ty_train, predictions\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t)\n",
    "\t\t\tgrads = inner_tape.gradient(loss, model.trainable_variables)\n",
    "\t\t\tfor var, grad in zip(model.trainable_variables, grads):\n",
    "\t\t\t\tif grad is not None:\n",
    "\t\t\t\t\tvar.assign_sub(inner_lr * grad)\n",
    "\n",
    "\t\tpredictions = model(x_train, training=True)\n",
    "\t\touter_loss = tf.reduce_mean(\n",
    "\t\t\ttf.keras.losses.sparse_categorical_crossentropy(y_train, predictions)\n",
    "\t\t)\n",
    "\n",
    "\touter_grads = outer_tape.gradient(outer_loss, model.trainable_variables)\n",
    "\treturn outer_loss, outer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HE_encrypt(wtarray):\n",
    "\tcwt = []\n",
    "\tfor layer in wtarray:\n",
    "\t\tflat_array = layer.astype(np.float64).flatten()\n",
    "\n",
    "\t\tchunks = np.array_split(flat_array, (len(flat_array) + 2**10 - 1) // 2**10)\n",
    "\t\tclayer = []\n",
    "\t\t\n",
    "\t\tfor chunk in chunks:\n",
    "\t\t\tptxt = HE.encodeFrac(chunk)\n",
    "\t\t\tctxt = HE.encryptPtxt(ptxt)\n",
    "\t\t\tclayer.append(ctxt)\n",
    "\t\tcwt.append(clayer.copy())\n",
    "\t\t\n",
    "\treturn cwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_message_sym_AES(key, message):\n",
    "\tserialized_obj = pickle.dumps(message)\n",
    "\n",
    "\tiv = nacl.utils.random(16)\n",
    "\n",
    "\tpadder = padding.PKCS7(algorithms.AES.block_size).padder()\n",
    "\tpadded_obj = padder.update(serialized_obj) + padder.finalize()\n",
    "\n",
    "\tcipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n",
    "\tencryptor = cipher.encryptor()\n",
    "\tciphertext = encryptor.update(padded_obj) + encryptor.finalize()\n",
    "\n",
    "\treturn iv + ciphertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_message_sym_AES(key , ciphertext):\n",
    "\tiv = ciphertext[:16]\n",
    "\tciphertext = ciphertext[16:]\n",
    "\n",
    "\tcipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n",
    "\tdecryptor = cipher.decryptor()\n",
    "\tpadded_obj = decryptor.update(ciphertext) + decryptor.finalize()\n",
    "\n",
    "\tunpadder = padding.PKCS7(algorithms.AES.block_size).unpadder()\n",
    "\tunpadded_obj = unpadder.update(padded_obj) + unpadder.finalize()\n",
    "\n",
    "\treturn pickle.loads(unpadded_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wts_ckks(encrypted_wts):\n",
    "\tres_wts = []\n",
    "\tprint(len(encrypted_wts) )\n",
    "\tprint(len(encrypted_wts[0]) )\n",
    "\tfor j in range(len(encrypted_wts[0])):\n",
    "\t\tlayer = []\n",
    "\t\tfor k in range(len(encrypted_wts[0][j])):\n",
    "\t\t\ttmp = encrypted_wts[0][j][k].copy()\n",
    "\n",
    "\t\t\tfor i in range(1, len(encrypted_wts)):\n",
    "\t\t\t\ttmp = tmp + encrypted_wts[i][j][k]\n",
    "\n",
    "\t\t\ttmp = tmp / len(encrypted_wts)\n",
    "\t\t\tlayer.append(tmp)\n",
    "\n",
    "\t\tres_wts.append(layer.copy())\n",
    "\t\t\n",
    "\treturn res_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_wts_ckks(encrypted_wts):\n",
    "\tdecrypted_wts = []\n",
    "\twtarray = dummy_model.get_weights()\n",
    "\n",
    "\tfor layer_wts , layer in zip(encrypted_wts , wtarray):\n",
    "\t\tdecrypted_layer = []\n",
    "\t\tflat_array = layer.astype(np.float64).flatten()\n",
    "\t\tchunks = np.array_split(flat_array, (len(flat_array) + 2**13 - 1) // 2**13)\n",
    "\n",
    "\t\tfor chunk , cchunk in zip(chunks , layer_wts):\n",
    "\t\t\tdecrypted_chunk = HE.decryptFrac(cchunk)\n",
    "\t\t\toriginal_chunk_size  = len(chunk)\n",
    "\t\t\tdecrypted_chunk = decrypted_chunk[:original_chunk_size]\n",
    "\t\t\t\n",
    "\t\t\tdecrypted_layer.append(decrypted_chunk)\n",
    "\t\t\t\n",
    "\t\tdecrypted_layer = np.concatenate(decrypted_layer)\n",
    "\t\tdecrypted_layer = decrypted_layer.reshape(layer.shape)\n",
    "\t\tdecrypted_wts.append(decrypted_layer)\n",
    "\t\t\n",
    "\treturn decrypted_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wts(wts):\n",
    "\tpeeled_wts = []\n",
    "\tfor client_id in range(no_clients):\n",
    "\t\tsesion_key = decrypt_symmetric_key(agg_pvt_key , agg_sesion_keys[client_id])\n",
    "\t\tpeeled_wt = decrypt_message_sym_AES(sesion_key, wts[client_id])\n",
    "\t\tpeeled_wts.append(peeled_wt)\n",
    "\tres_wts = aggregate_wts_ckks(peeled_wts)\n",
    "\treturn res_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = fml.clients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_sesion_keys = [0 for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_lr = 0.001\n",
    "num_inner_updates = 1\n",
    "outer_lr = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [[] for i in range(no_clients)]\n",
    "losses = [[] for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_session_keys = [0 for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_wts = [0 for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:603: UserWarning: `model.compiled_loss()` is deprecated. Instead, use `model.compute_loss(x, y, y_pred, sample_weight)`.\n",
      "  warnings.warn(\n",
      "/home/cs-lab-12/miniconda3/envs/f39/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:578: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n",
      "```\n",
      "for metric in self.metrics:\n",
      "    metric.update_state(y, y_pred)\n",
      "```\n",
      "\n",
      "  return self._compiled_metrics_update_state(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7284 - loss: -0.0391 - val_accuracy: 0.9523 - val_loss: -0.2960\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.7294 - loss: -0.7090 - val_accuracy: 0.9673 - val_loss: -1.2801\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7537 - loss: -0.2594 - val_accuracy: 0.9571 - val_loss: -0.2118\n",
      "101725776\n",
      "3\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:41<01:23, 41.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.6239 - loss: -0.7810 - val_accuracy: 0.9391 - val_loss: -1.2682\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.5743 - loss: -0.8244 - val_accuracy: 0.9297 - val_loss: -1.7339\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.5770 - loss: -1.7001 - val_accuracy: 0.9180 - val_loss: -2.8679\n",
      "101725776\n",
      "3\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [01:20<00:39, 39.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.6057 - loss: -1.3997 - val_accuracy: 0.9183 - val_loss: -2.5071\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.5702 - loss: -1.1015 - val_accuracy: 0.9292 - val_loss: -1.8288\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6192 - loss: -1.2342 - val_accuracy: 0.9154 - val_loss: -2.2302\n",
      "101725776\n",
      "3\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:58<00:00, 39.60s/it]\n"
     ]
    }
   ],
   "source": [
    "for r in tqdm(range(epochs)):\n",
    "\tfor client_id , (client , client_dataset) in enumerate(zip(fml.clients , dataset_parts)):\n",
    "\t\tmodel = client\n",
    "\t\tx_train, y_train = client_dataset\n",
    "\n",
    "\t\touter_loss, outer_grads = maml_train_step(\n",
    "\t\t\t\tmodel, x_train, y_train, inner_lr, num_inner_updates\n",
    "\t\t\t)\n",
    "\t\toptimizer = tf.keras.optimizers.Adam(learning_rate=outer_lr)\n",
    "\t\toptimizer.apply_gradients(zip(outer_grads, model.trainable_variables))\n",
    "\t\thistory = model.evaluate(\n",
    "\t\t\tx_train,\n",
    "\t\t\ty_train,\n",
    "\t\t\tbatch_size=64,\n",
    "\t\t\tverbose=0,\n",
    "\t\t)\n",
    "\t\taccuracies[client_id].append(history[1])\n",
    "\t\tlosses[client_id].append(history[0])\n",
    "\t\ttrained_weights = model.get_weights()\n",
    "\n",
    "\t\tsession_key = nacl_session_keygen()\n",
    "\t\tclient_session_keys[client_id] = session_key\n",
    "\t\tenc_session_key = encrypt_symmetric_key(agg_pub_key , session_key)\n",
    "\t\tagg_sesion_keys[client_id] = enc_session_key\n",
    "\n",
    "\t\tHe_ciphertext = HE_encrypt(trained_weights)\n",
    "\t\tsym_ctxt = encrypt_message_sym_AES(session_key , He_ciphertext)\n",
    "\t\tenc_wts[client_id] = sym_ctxt\n",
    "\tprint(len(enc_wts[0]))\n",
    "\tagg_wts = aggregate_wts(enc_wts)\n",
    "\n",
    "\tfor client_id , client in enumerate(fml.clients):\n",
    "\t\tnew_wts = decrypt_wts_ckks(agg_wts)        \n",
    "\n",
    "\t\tclient.set_weights(new_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.750074863433838, 0.8950625061988831]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_parts[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmlee_accuracies = accuracies\n",
    "fmlee_losses = losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CKKS scheme...\n",
      "Generating context for CKKS scheme...\n",
      "Generating keys for CKKS scheme...\n",
      "CKKS scheme initialized.\n",
      "Initializing clients...\n",
      "Initializing model for client 0...\n",
      "Client 0 initialized.\n",
      "Initializing model for client 1...\n",
      "Client 1 initialized.\n",
      "Initializing model for client 2...\n",
      "Client 2 initialized.\n",
      "Generating asymmetric keys...\n",
      "Generating private key...\n",
      "Private key generated.\n",
      "Public key generated.\n",
      "Initialization complete.\n"
     ]
    }
   ],
   "source": [
    "no_fml = FMLEE(no_clients, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [[] for i in range(no_clients)]\n",
    "losses = [[] for i in range(no_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wts_noenc(weight_list):\n",
    "    n_models = len(weight_list)\n",
    "    # Initialize the aggregated weights with zeros of the same shape as the first model's weights\n",
    "    aggregated_weights = [np.zeros_like(w) for w in weight_list[0]]\n",
    "\n",
    "    # Sum the weights for each layer\n",
    "    for weights in weight_list:\n",
    "        for i, weight in enumerate(weights):\n",
    "            aggregated_weights[i] += weight\n",
    "\n",
    "    # Divide by the number of models to get the average\n",
    "    for i in range(len(aggregated_weights)):\n",
    "        aggregated_weights[i] /= n_models\n",
    "\n",
    "    return aggregated_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6036 - loss: -1.6955 - val_accuracy: 0.9324 - val_loss: -3.1265\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.5893 - loss: -1.2142 - val_accuracy: 0.9408 - val_loss: -2.2577\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6431 - loss: -2.0113 - val_accuracy: 0.9279 - val_loss: -3.0689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:32<01:05, 32.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9255 - loss: -2.7088 - val_accuracy: 0.9637 - val_loss: -2.8026\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9318 - loss: -2.8509 - val_accuracy: 0.9606 - val_loss: -3.2036\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9265 - loss: -2.7121 - val_accuracy: 0.9622 - val_loss: -2.7586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [01:05<00:32, 32.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9666 - loss: -3.2297 - val_accuracy: 0.9737 - val_loss: -3.0848\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9673 - loss: -3.2801 - val_accuracy: 0.9674 - val_loss: -2.9962\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9646 - loss: -3.2153 - val_accuracy: 0.9775 - val_loss: -3.4626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:38<00:00, 32.87s/it]\n"
     ]
    }
   ],
   "source": [
    "for r in tqdm(range(epochs)):\n",
    "    for client_id, (client, client_dataset) in enumerate(\n",
    "        zip(fml.clients, dataset_parts)\n",
    "    ):\n",
    "        model = client\n",
    "        x_train, y_train = client_dataset\n",
    "\n",
    "        outer_loss, outer_grads = maml_train_step(\n",
    "            model, x_train, y_train, inner_lr, num_inner_updates\n",
    "        )\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=outer_lr)\n",
    "        optimizer.apply_gradients(zip(outer_grads, model.trainable_variables))\n",
    "        history = model.evaluate(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            batch_size=64,\n",
    "            verbose=0,\n",
    "        )\n",
    "        accuracies[client_id].append(history[1])\n",
    "        losses[client_id].append(history[0])\n",
    "        trained_weights = model.get_weights()\n",
    "\n",
    "        enc_wts[client_id] = trained_weights\n",
    "    agg_wts = aggregate_wts_noenc(enc_wts)\n",
    "\n",
    "    for client_id, client in enumerate(fml.clients):\n",
    "        client.set_weights(agg_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.0400 - loss: 3.0483\n",
      "{'accuracy': [0.03999999910593033], 'loss': [3.141841173171997]}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "num_samples = 100\n",
    "num_classes = 10\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# Generate random images and labels\n",
    "x_dummy = np.random.rand(num_samples, *input_shape).astype(np.float32)\n",
    "y_dummy = np.random.randint(0, num_classes, num_samples)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_dummy = tf.keras.utils.to_categorical(y_dummy, num_classes)\n",
    "\n",
    "# Load MobileNetV2 with pre-trained weights and exclude the top layers\n",
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Add custom top layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Create the full model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model for one epoch\n",
    "history = model.fit(x_dummy, y_dummy, epochs=1, batch_size=32)\n",
    "\n",
    "# Print the training history\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    UpSampling2D,\n",
    "    Concatenate,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Define U-Net architecture\n",
    "def unet(input_size=(128, 128, 1)):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    conv1 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "    conv3 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n",
    "    conv4 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(1024, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n",
    "    conv5 = Conv2D(1024, (3, 3), activation=\"relu\", padding=\"same\")(conv5)\n",
    "\n",
    "    # Decoder\n",
    "    up6 = UpSampling2D(size=(2, 2))(conv5)\n",
    "    up6 = Conv2D(512, (2, 2), activation=\"relu\", padding=\"same\")(up6)\n",
    "    merge6 = Concatenate()([conv4, up6])\n",
    "    conv6 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(merge6)\n",
    "    conv6 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(conv6)\n",
    "\n",
    "    up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    up7 = Conv2D(256, (2, 2), activation=\"relu\", padding=\"same\")(up7)\n",
    "    merge7 = Concatenate()([conv3, up7])\n",
    "    conv7 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(merge7)\n",
    "    conv7 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(conv7)\n",
    "\n",
    "    up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    up8 = Conv2D(128, (2, 2), activation=\"relu\", padding=\"same\")(up8)\n",
    "    merge8 = Concatenate()([conv2, up8])\n",
    "    conv8 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(merge8)\n",
    "    conv8 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(conv8)\n",
    "\n",
    "    up9 = UpSampling2D(size=(2, 2))(conv8)\n",
    "    up9 = Conv2D(64, (2, 2), activation=\"relu\", padding=\"same\")(up9)\n",
    "    merge9 = Concatenate()([conv1, up9])\n",
    "    conv9 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(merge9)\n",
    "    conv9 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(conv9)\n",
    "    conv9 = Conv2D(2, (3, 3), activation=\"relu\", padding=\"same\")(conv9)\n",
    "    conv10 = Conv2D(1, (1, 1), activation=\"sigmoid\")(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a dummy dataset\n",
    "num_samples = 100\n",
    "input_shape = (128, 128, 1)\n",
    "\n",
    "# Generate random images and labels\n",
    "x_dummy = np.random.rand(num_samples, *input_shape).astype(np.float32)\n",
    "y_dummy = np.random.randint(0, 2, (num_samples, 128, 128, 1)).astype(np.float32)\n",
    "\n",
    "unet_models = []\n",
    "for _ in range(no_clients):\n",
    "# Instantiate the U-Net model\n",
    "    model = unet(input_shape)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    unet_models.append(model)\n",
    "# Train the model for one epoch\n",
    "# history = model.fit(x_dummy, y_dummy, epochs=1, batch_size=8)\n",
    "\n",
    "# Print the training history\n",
    "# print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time as tm\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Initialize total time counters\n",
    "# total_training_time_unet = 0.0\n",
    "# total_encryption_time_unet = 0.0\n",
    "# total_aggregation_time_unet = 0.0\n",
    "# total_decryption_time_unet = 0.0\n",
    "\n",
    "# for r in tqdm(range(1)):\n",
    "#     round_start_time_unet = tm.time()  # Start time for the round\n",
    "\n",
    "#     for client_id, (client, client_dataset) in enumerate(\n",
    "#         zip(unet_models, dataset_parts)\n",
    "#     ):\n",
    "#         model = client\n",
    "#         x_train, y_train = client_dataset\n",
    "\n",
    "#         # Train the model\n",
    "#         training_start_time_unet = tm.time()\n",
    "#         history = model.fit(x_dummy, y_dummy, epochs=1, batch_size=8)\n",
    "#         training_end_time_unet = tm.time()\n",
    "\n",
    "#         trained_weights = model.get_weights()\n",
    "\n",
    "#         # Encryption\n",
    "#         unet_enc_start_time_unet = tm.time()\n",
    "#         session_key = nacl_session_keygen()\n",
    "#         client_session_keys[client_id] = session_key\n",
    "#         enc_session_key = encrypt_symmetric_key(agg_pub_key, session_key)\n",
    "#         agg_sesion_keys[client_id] = enc_session_key\n",
    "\n",
    "#         He_ciphertext = HE_encrypt(trained_weights)\n",
    "#         sym_ctxt = encrypt_message_sym_AES(session_key, He_ciphertext)\n",
    "#         enc_wts[client_id] = sym_ctxt\n",
    "#         unet_enc_end_time_unet = tm.time()\n",
    "\n",
    "#         # Update total training and encryption times\n",
    "#         total_training_time_unet += training_end_time_unet - training_start_time_unet\n",
    "#         total_encryption_time_unet += unet_enc_end_time_unet - unet_enc_start_time_unet\n",
    "\n",
    "#     print(len(enc_wts[0]))\n",
    "\n",
    "#     # Aggregation\n",
    "#     unet_agg_start_time_unet = tm.time()\n",
    "#     agg_wts = aggregate_wts(enc_wts)\n",
    "#     unet_agg_end_time_unet = tm.time()\n",
    "\n",
    "#     total_aggregation_time_unet += unet_agg_end_time_unet - unet_agg_start_time_unet\n",
    "\n",
    "#     for client_id, client in enumerate(fml.clients):\n",
    "#         # Decryption\n",
    "#         unet_dec_start_time_unet = tm.time()\n",
    "#         new_wts = decrypt_wts_ckks(agg_wts)\n",
    "#         unet_dec_end_time_unet = tm.time()\n",
    "\n",
    "#         client.set_weights(new_wts)\n",
    "\n",
    "#         total_decryption_time_unet += unet_dec_end_time_unet - unet_dec_start_time_unet\n",
    "\n",
    "#     round_end_time_unet = tm.time()  # End time for the round\n",
    "\n",
    "#     total_round_time_unet = round_end_time_unet - round_start_time_unet\n",
    "\n",
    "#     print(f\"Total training time for round: {total_training_time_unet:.2f} seconds\")\n",
    "#     print(f\"Total encryption time for round: {total_encryption_time_unet:.2f} seconds\")\n",
    "#     print(\n",
    "#         f\"Total aggregation time for round: {total_aggregation_time_unet:.2f} seconds\"\n",
    "#     )\n",
    "#     print(f\"Total decryption time for round: {total_decryption_time_unet:.2f} seconds\")\n",
    "#     print(f\"Total round time: {total_round_time_unet:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 4s/step - accuracy: 0.0841 - loss: 3.5777\n",
      "{'accuracy': [0.07999999821186066], 'loss': [3.977829933166504]}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Create a dummy dataset\n",
    "num_samples = 100\n",
    "num_classes = 10\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# Generate random images and labels\n",
    "x_dummy = np.random.rand(num_samples, *input_shape).astype(np.float32)\n",
    "y_dummy = np.random.randint(0, num_classes, num_samples)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_dummy = tf.keras.utils.to_categorical(y_dummy, num_classes)\n",
    "\n",
    "# Load ResNet50 with pre-trained weights and exclude the top layers\n",
    "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Add custom top layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "resnet_models = []\n",
    "for _ in range(no_clients):\n",
    "    # Create the full model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    resnet_models.append(model)\n",
    "    # \n",
    "# Create the full model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model for one epoch\n",
    "# history = model.fit(x_dummy, y_dummy, epochs=1, batch_size=32)\n",
    "\n",
    "# # Print the training history\n",
    "# print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_training_time_resnet = 0.0\n",
    "total_encryption_time_resnet = 0.0\n",
    "total_aggregation_time_resnet = 0.0\n",
    "total_decryption_time_resnet = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.0899 - loss: 2.9008\n"
     ]
    }
   ],
   "source": [
    "import time as tm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize total time counters\n",
    "total_training_time_resnet = 0.0\n",
    "total_encryption_time_resnet = 0.0\n",
    "total_aggregation_time_resnet = 0.0\n",
    "total_decryption_time_resnet = 0.0\n",
    "\n",
    "for r in tqdm(range(1)):\n",
    "    round_start_time_resnet = tm.time()  # Start time for the round\n",
    "\n",
    "    for client_id, (model, client_dataset) in enumerate(\n",
    "        zip(resnet_models, dataset_parts)\n",
    "    ):\n",
    "        # x_train, y_train = client_dataset\n",
    "\n",
    "        # Train the model\n",
    "        training_start_time_resnet = tm.time()\n",
    "        history = model.fit(x_dummy, y_dummy, epochs=1, batch_size=8)\n",
    "        training_end_time_resnet = tm.time()\n",
    "\n",
    "        trained_weights = model.get_weights()\n",
    "\n",
    "        # Encryption\n",
    "        resnet_enc_start_time_resnet = tm.time()\n",
    "        session_key = nacl_session_keygen()\n",
    "        client_session_keys[client_id] = session_key\n",
    "        enc_session_key = encrypt_symmetric_key(agg_pub_key, session_key)\n",
    "        agg_sesion_keys[client_id] = enc_session_key\n",
    "\n",
    "        He_ciphertext = HE_encrypt(trained_weights)\n",
    "        sym_ctxt = encrypt_message_sym_AES(session_key, He_ciphertext)\n",
    "        enc_wts[client_id] = sym_ctxt\n",
    "        resnet_enc_end_time_resnet = tm.time()\n",
    "\n",
    "        # Update total training and encryption times\n",
    "        total_training_time_resnet = (\n",
    "            training_end_time_resnet - training_start_time_resnet\n",
    "        )\n",
    "        total_encryption_time_resnet = (\n",
    "            resnet_enc_end_time_resnet - resnet_enc_start_time_resnet\n",
    "        )\n",
    "\n",
    "    print(len(enc_wts[0]))\n",
    "\n",
    "    # Aggregation\n",
    "    resnet_agg_start_time_resnet = tm.time()\n",
    "    agg_wts = aggregate_wts(enc_wts)\n",
    "    resnet_agg_end_time_resnet = tm.time()\n",
    "\n",
    "    total_aggregation_time_resnet = (\n",
    "        resnet_agg_end_time_resnet - resnet_agg_start_time_resnet\n",
    "    )\n",
    "\n",
    "    for client_id, client in enumerate(resnet_models):\n",
    "        # Decryption\n",
    "        resnet_dec_start_time_resnet = tm.time()\n",
    "        new_wts = decrypt_wts_ckks(agg_wts)\n",
    "        resnet_dec_end_time_resnet = tm.time()\n",
    "\n",
    "        client.set_weights(new_wts)\n",
    "\n",
    "        total_decryption_time_resnet = (\n",
    "            resnet_dec_end_time_resnet - resnet_dec_start_time_resnet\n",
    "        )\n",
    "\n",
    "    round_end_time_resnet = tm.time()  # End time for the round\n",
    "\n",
    "    total_round_time_resnet = round_end_time_resnet - round_start_time_resnet\n",
    "\n",
    "    print(f\"Total training time for round: {total_training_time_resnet:.2f} seconds\")\n",
    "    print(\n",
    "        f\"Total encryption time for round: {total_encryption_time_resnet:.2f} seconds\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Total aggregation time for round: {total_aggregation_time_resnet:.2f} seconds\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Total decryption time for round: {total_decryption_time_resnet:.2f} seconds\"\n",
    "    )\n",
    "    print(f\"Total round time: {total_round_time_resnet:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Create a dummy dataset\n",
    "num_samples = 100\n",
    "num_classes = 10\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# Generate random images and labels\n",
    "x_dummy = np.random.rand(num_samples, *input_shape).astype(np.float32)\n",
    "y_dummy = np.random.randint(0, num_classes, num_samples)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_dummy = tf.keras.utils.to_categorical(y_dummy, num_classes)\n",
    "\n",
    "# Load MobileNetV2 with pre-trained weights and exclude the top layers\n",
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Add custom top layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "no_clients = 10  # Number of clients\n",
    "\n",
    "mobilenet_models = []\n",
    "for _ in range(no_clients):\n",
    "    # Create the full model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    mobilenet_models.append(model)\n",
    "\n",
    "# # Example of training the first model for one epoch\n",
    "# history = mobilenet_models[0].fit(x_dummy, y_dummy, epochs=1, batch_size=32)\n",
    "\n",
    "# # Print the training history for the first model\n",
    "# print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as tm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize total time counters\n",
    "total_training_time_mobilenetv2 = 0.0\n",
    "total_encryption_time_mobilenetv2 = 0.0\n",
    "total_aggregation_time_mobilenetv2 = 0.0\n",
    "total_decryption_time_mobilenetv2 = 0.0\n",
    "\n",
    "for r in tqdm(range(1)):\n",
    "    round_start_time_mobilenetv2 = tm.time()  # Start time for the round\n",
    "\n",
    "    for client_id, (model, client_dataset) in enumerate(\n",
    "        zip(mobilenet_models, dataset_parts)\n",
    "    ):\n",
    "        # x_train, y_train = client_dataset\n",
    "\n",
    "        # Train the model\n",
    "        training_start_time_mobilenetv2 = tm.time()\n",
    "        history = model.fit(x_dummy, y_dummy, epochs=1, batch_size=8)\n",
    "        training_end_time_mobilenetv2 = tm.time()\n",
    "\n",
    "        trained_weights = model.get_weights()\n",
    "\n",
    "        # Encryption\n",
    "        mobilenetv2_enc_start_time_mobilenetv2 = tm.time()\n",
    "        session_key = nacl_session_keygen()\n",
    "        client_session_keys[client_id] = session_key\n",
    "        enc_session_key = encrypt_symmetric_key(agg_pub_key, session_key)\n",
    "        agg_sesion_keys[client_id] = enc_session_key\n",
    "\n",
    "        He_ciphertext = HE_encrypt(trained_weights)\n",
    "        sym_ctxt = encrypt_message_sym_AES(session_key, He_ciphertext)\n",
    "        enc_wts[client_id] = sym_ctxt\n",
    "        mobilenetv2_enc_end_time_mobilenetv2 = tm.time()\n",
    "\n",
    "        # Update total training and encryption times\n",
    "        total_training_time_mobilenetv2 = (\n",
    "            training_end_time_mobilenetv2 - training_start_time_mobilenetv2\n",
    "        )\n",
    "        total_encryption_time_mobilenetv2 = (\n",
    "            mobilenetv2_enc_end_time_mobilenetv2\n",
    "            - mobilenetv2_enc_start_time_mobilenetv2\n",
    "        )\n",
    "\n",
    "    print(len(enc_wts[0]))\n",
    "\n",
    "    # Aggregation\n",
    "    mobilenetv2_agg_start_time_mobilenetv2 = tm.time()\n",
    "    agg_wts = aggregate_wts(enc_wts)\n",
    "    mobilenetv2_agg_end_time_mobilenetv2 = tm.time()\n",
    "\n",
    "    total_aggregation_time_mobilenetv2 = (\n",
    "        mobilenetv2_agg_end_time_mobilenetv2 - mobilenetv2_agg_start_time_mobilenetv2\n",
    "    )\n",
    "\n",
    "    for client_id, client in enumerate(fml.clients):\n",
    "        # Decryption\n",
    "        mobilenetv2_dec_start_time_mobilenetv2 = tm.time()\n",
    "        new_wts = decrypt_wts_ckks(agg_wts)\n",
    "        mobilenetv2_dec_end_time_mobilenetv2 = tm.time()\n",
    "\n",
    "        client.set_weights(new_wts)\n",
    "\n",
    "        total_decryption_time_mobilenetv2 += (\n",
    "            mobilenetv2_dec_end_time_mobilenetv2\n",
    "            - mobilenetv2_dec_start_time_mobilenetv2\n",
    "        )\n",
    "\n",
    "    round_end_time_mobilenetv2 = tm.time()  # End time for the round\n",
    "\n",
    "    total_round_time_mobilenetv2 = (\n",
    "        round_end_time_mobilenetv2 - round_start_time_mobilenetv2\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Total training time for round: {total_training_time_mobilenetv2:.2f} seconds\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Total encryption time for round: {total_encryption_time_mobilenetv2:.2f} seconds\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Total aggregation time for round: {total_aggregation_time_mobilenetv2:.2f} seconds\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Total decryption time for round: {total_decryption_time_mobilenetv2:.2f} seconds\"\n",
    "    )\n",
    "    print(f\"Total round time: {total_round_time_mobilenetv2:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data: Replace these with actual times collected from your training runs\n",
    "total_encryption_time_resnet = 120.0  # Example time in seconds\n",
    "total_decryption_time_resnet = 80.0\n",
    "total_aggregation_time_resnet = 60.0\n",
    "\n",
    "total_encryption_time_mobilenetv2 = 110.0\n",
    "total_decryption_time_mobilenetv2 = 70.0\n",
    "total_aggregation_time_mobilenetv2 = 50.0\n",
    "\n",
    "# Data\n",
    "models = [\"ResNet\", \"MobileNetV2\"]\n",
    "encryption_times = [total_encryption_time_resnet, total_encryption_time_mobilenetv2]\n",
    "decryption_times = [total_decryption_time_resnet, total_decryption_time_mobilenetv2]\n",
    "aggregation_times = [total_aggregation_time_resnet, total_aggregation_time_mobilenetv2]\n",
    "\n",
    "# Bar width\n",
    "bar_width = 0.25\n",
    "\n",
    "# Positions of the bars on the x-axis\n",
    "r1 = range(len(models))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "r3 = [x + bar_width for x in r2]\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Bars for encryption times\n",
    "plt.bar(\n",
    "    r1,\n",
    "    encryption_times,\n",
    "    color=\"b\",\n",
    "    width=bar_width,\n",
    "    edgecolor=\"grey\",\n",
    "    label=\"Encryption Time\",\n",
    ")\n",
    "\n",
    "# Bars for decryption times\n",
    "plt.bar(\n",
    "    r2,\n",
    "    decryption_times,\n",
    "    color=\"g\",\n",
    "    width=bar_width,\n",
    "    edgecolor=\"grey\",\n",
    "    label=\"Decryption Time\",\n",
    ")\n",
    "\n",
    "# Bars for aggregation times\n",
    "plt.bar(\n",
    "    r3,\n",
    "    aggregation_times,\n",
    "    color=\"r\",\n",
    "    width=bar_width,\n",
    "    edgecolor=\"grey\",\n",
    "    label=\"Aggregation Time\",\n",
    ")\n",
    "\n",
    "# Adding the labels and title\n",
    "plt.xlabel(\"Models\", fontweight=\"bold\")\n",
    "plt.ylabel(\"Time (seconds)\", fontweight=\"bold\")\n",
    "plt.title(\"Comparison of Encryption, Decryption, and Aggregation Times\")\n",
    "plt.xticks([r + bar_width for r in range(len(models))], models)\n",
    "\n",
    "# Adding the legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "he39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
